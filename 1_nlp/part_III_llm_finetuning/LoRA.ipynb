{"cells":[{"cell_type":"markdown","source":["Natural Language Processing Tutorial\n","======\n","\n","This is the tutorial of the 2024 [Mediterranean Machine Learning Summer School](https://www.m2lschool.org/) on Natural Language Processing!\n","\n","This tutorial will explore the fundamental aspects of Natural Language Processing (NLP). Basic Python programming skills are expected.\n","Prior knowledge of standard NLP techniques (e.g. text tokenization and classification with ML) is beneficial but optional when working through the notebooks as they assume minimal prior knowledge.\n","\n","This tutorial combines detailed analysis and development of essential NLP concepts via custom (i.e. from scratch) implementations. Other necessary NLP components will be developed using PyTorch's NLP library implementations. As a result, the tutorial offers deep understanding and facilitates easy usage in future applications.\n","\n","## Outline\n","\n","* Part I: Introduction to Text Tokenization and Classification\n","  *  Text Classification: Simple Classifier\n","  *  Text Classification: Encoder-only Transformer\n","\n","* Part II: Introduction to Decoder-only Transformer and Sparse Mixture of Experts Architecture\n","  *  Text Generation: Decoder-only Transformer\n","  *  Text Generation: Decoder-only Transformer + MoE\n","\n","* Part III: Introduction to Parameter Efficient Fine-tuning\n","  *  Fine-tuning the full Pre-trained Models\n","  *  Fine-tuning using Low-Rank Adaptation of Large Language Models (LoRA)\n","\n","## Notation\n","\n","* Sections marked with [ðŸ“š] contain cells that you should read, modify and complete to understand how your changes alter the obtained results.\n","* External resources are mentioned with [âœ¨]. These provide valuable supplementary information for this tutorial and offer opportunities for further in-depth exploration of the topics covered.\n","\n","\n","## Libraries\n","\n","This tutorial leverages [PyTorch](https://pytorch.org/) for neural network implementation and training, complemented by standard Python libraries for data processing and the [Hugging Face](https://huggingface.co/) datasets library for accessing NLP resources.\n","\n","GPU access is recommended for optimal performance, particularly for model training and text generation. While all code can run on CPU, a CUDA-enabled environment will significantly speed up these processes.\n","\n","## Credits\n","\n","The tutorial is created by:\n","\n","* [Luca Herranz-Celotti](http://LuCeHe.github.io)\n","* [Georgios Peikos](https://www.linkedin.com/in/peikosgeorgios/)\n","\n","It is inspired by and synthesizes various online resources, which are cited throughout for reference and further reading.\n","\n","## Note for Colab users\n","\n","To grab a GPU (if available), make sure you go to `Edit -> Notebook settings` and choose a GPU under `Hardware accelerator`\n","\n"],"metadata":{"id":"F45HFdoiriet"}},{"cell_type":"markdown","source":["## Part III: Introduction to Parameter Efficient Fine-tuning\n","\n","We show how to adapt a model that has been pre-trained on a lot of data, can be adapted to be used in a downstream task, by fine-tuning it on a target dataset. The first idea could be to consider adapting all the weights of the network to the new task, but this can be resource intensive. This could lead us to decide that we can freeze all the weights, except the final output linear layer. We will see that this results in faster training, but also in worse performance on our target task. Finally we introduce a newer way of thinking, Parameter Efficient Fine-Tuning (PEFT) and one approach in that family, LoRA, that will provide us with a way to improve performance in a fine-tuning task, while being less resource intensive.\n"],"metadata":{"id":"GcFmNhCLZbtp"}},{"cell_type":"markdown","metadata":{"id":"Voy7Ivrr0MSi"},"source":["##Step 1: Load Packages\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XNg1OQ7hUVQM","collapsed":true},"outputs":[],"source":["!pip install peft datasets evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qFNm9eSAUD8l"},"outputs":[],"source":["import math\n","import torch\n","import torch.nn as nn\n","\n","from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification\n","from transformers.utils import PushToHubMixin\n","\n","from peft.tuners.lora.layer import dispatch_default, Linear\n","from peft.tuners.tuners_utils import BaseTunerLayer\n","from peft import LoraConfig, PeftModel, LoraModel, get_peft_model\n","from datasets import load_dataset\n","\n","import numpy as np\n","import evaluate\n","from transformers import TrainingArguments, Trainer"]},{"cell_type":"markdown","source":["We will fine-tune the âœ¨ [BERT](https://arxiv.org/pdf/1810.04805) architecture, a well known language classification architecture built based on the Transformer encoder. The pre-trained model is openly available at different sources. We will focus on the HuggingFace library, since it has become a standard for Large Language Models, and it includes a large number of convenient tools for language processing and generation."],"metadata":{"id":"PGW42rGoqypH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pdDrMfPbyUky"},"outputs":[],"source":["model_name_or_path = \"google-bert/bert-base-cased\"\n","tokenizer_name_or_path = \"google-bert/bert-base-cased\""]},{"cell_type":"markdown","metadata":{"id":"CA4zbqo40Qnj"},"source":["##ðŸ“š Step 2: Load Dataset"]},{"cell_type":"markdown","metadata":{"id":"WV8gPKOeyXi4"},"source":["Let's pick a dataset and use the tokenizer that corresponds to the BERT model. The âœ¨ [Yelp reviews dataset](https://huggingface.co/datasets/Yelp/yelp_review_full) consists of reviews from Yelp, and each review has a number of stars between one and five. The neural network will see the review at the input, and will have to predict the number of stars that correspond to that review.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q9WzUAxwyRyH","collapsed":true},"outputs":[],"source":["# EXERCISE: load the yelp_review_full dataset using load_dataset\n","dataset =\n","\n","print(dataset)\n","print(dataset[\"train\"][100])\n","\n","# EXERCISE: load the BERT tokenizer with AutoTokenizer\n","tokenizer =\n","\n","def tokenize_function(examples):\n","    # EXERCISE: pad to max length and truncate sentences\n","    return tokenizer(examples[\"text\"],\n","\n","tokenized_datasets = dataset.map(tokenize_function, batched=True)\n","small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n","small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"]},{"cell_type":"markdown","metadata":{"id":"wbhu7MQT01w_"},"source":["## ðŸ“š Step 3: Define Training and Evaluation Loop\n","Let's standardize the training and evaluation loop, so we can better appreciate the difference in the final result between the three finetuning techniques explained."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"10j0UC3502Ul"},"outputs":[],"source":["def train_and_evaluate(model, max_steps=-1, num_train_epochs=2, learning_rate=5e-5):\n","    metric = evaluate.load(\"accuracy\")\n","\n","    def compute_metrics(eval_pred):\n","        logits, labels = eval_pred\n","        # EXERCISE: the greedy prediction is the argmax of the logits\n","        predictions =\n","        return metric.compute(predictions=predictions, references=labels)\n","\n","    training_args = TrainingArguments(\n","        output_dir=\"test_trainer\",\n","        num_train_epochs=num_train_epochs,\n","        max_steps=max_steps,\n","        learning_rate=learning_rate,\n","        label_names=[\"labels\"],\n","    )\n","\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=small_train_dataset,\n","        eval_dataset=small_eval_dataset,\n","        compute_metrics=compute_metrics,\n","    )\n","\n","    train_metrics = trainer.train()\n","    print(train_metrics)\n","    eval_metrics = trainer.evaluate()\n","    print(eval_metrics)"]},{"cell_type":"markdown","metadata":{"id":"_5bPfW1n1NOu"},"source":["We also introduce an auxiliary function to count the number of trainable parameters in each case."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PE2Ro46X1Mob"},"outputs":[],"source":["def print_trainable_parameters(model):\n","    \"\"\"\n","    Prints the number of trainable parameters in the model.\n","    \"\"\"\n","    trainable_params = 0\n","    all_param = 0\n","    for _, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","    print(\n","        f\"trainable params: {trainable_params} || all params: {all_param} || trainable: {100 * trainable_params / all_param:.2f}%\"\n","    )"]},{"cell_type":"markdown","metadata":{"id":"nNfNGUSuyP_t"},"source":["## ðŸ“š Step 4: Full Finetuning\n","\n","The simplest possibility is to fine-tune all the model, the pre-trained BERT, but also the new linear classifier on top. This might usually achieve the best final accuracy, but it results in slow fine-tuning. This is because all the matrices in the model have to be updated, which can be very large and consume a lot of memory."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"st1O7DuM1a2v"},"outputs":[],"source":["model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, num_labels=5)\n","print_trainable_parameters(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ijVpFeyR1eaq"},"outputs":[],"source":["# EXERCISE: explore learning rates in the set [5e-2, 5e-3, 5e-4, 5e-5] to find the best\n","# one with this configuration\n","train_and_evaluate(model, learning_rate="]},{"cell_type":"markdown","metadata":{"id":"AS2k9B120gOv"},"source":["## ðŸ“š Step 5: Head Finetuning\n","\n","Another possibility is to fix the weights of the pre-trained BERT, and fine-tune only the head, the linear classifier that HuggingFace has placed on top when we say `num_labels=5`. This will drastically reduce the number of trainable parameters, and therefore it will significantly speed up fine-tuning."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eJggghVeYPUj"},"outputs":[],"source":["model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, num_labels=5)\n","\n","# EXERCISE: set as trainable only the parameters of the classifier\n","for name, param in model.named_parameters():\n","\n","print_trainable_parameters(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nZsfd65FfBea"},"outputs":[],"source":["# EXERCISE: explore learning rates in the set [5e-2, 5e-3, 5e-4, 5e-5] to find the best\n","# one with this configuration\n","train_and_evaluate(model, learning_rate="]},{"cell_type":"markdown","metadata":{"id":"UiAx9Rb90idU"},"source":["## ðŸ“š Step 6: LoRA Finetuning\n","\n","A newer line of research, called Parameter Efficient Fine-Tuning (PEFT) attempts to figure out different techniques to drastically reduce the number of parameters to fine-tune, and still achieve good performance. One of the most popular options is called âœ¨ [LoRA](https://arxiv.org/pdf/2106.09685), for Low-Rank adaptation of Language Models. It consists on constructing the new matrices as $\\theta = \\hat{\\theta} + A^TB$, where $\\theta$ is the new matrix of our model, the pre-trained weights $\\hat{\\theta}$ are kept fixed, and only an additive component made up by multiplying two smaller matrices $A,B$ is learned. This drastically reduces the number of parameters to train, if $A,B$ are chosen appropriately.\n","\n","<img src=\"https://heidloff.net/assets/img/2023/08/lora.png\" alt=\"drawing\" width=\"50%\"/>\n","\n","\n","The speed up is noticeable with BERT, and becomes more significant for larger models. The matrices $A,B$ have size $A,B\\in\t\\mathbb{R}^{r\\times N}$, where the size of the original matrix was $\\theta,\\hat{\\theta}\\in\t\\mathbb{R}^{N\\times N}$.\n","\n","Now, let's first define the hyper-parameters of our LoRA:"]},{"cell_type":"code","source":["peft_config = LoraConfig(\n","    r=16,\n","    lora_alpha=32,\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")"],"metadata":{"id":"lC2BGL8Z7EX8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then let's define the LoRA layer itself."],"metadata":{"id":"J1hYxrb67KaV"}},{"cell_type":"code","source":["class CustomLinearLoRA(Linear):\n","    def update_layer(\n","            self, adapter_name, r, lora_alpha, lora_dropout, init_lora_weights, use_rslora=False, use_dora=False\n","    ):\n","        # This code works for linear layers, override for other layer types\n","        if r <= 0:\n","            raise ValueError(f\"`r` should be a positive integer value but the value passed is {r}\")\n","\n","        self.r[adapter_name] = r\n","        self.lora_alpha[adapter_name] = lora_alpha\n","\n","        # EXERCISE: define a dropout layer\n","        lora_dropout_layer =\n","\n","        self.lora_dropout.update(nn.ModuleDict({adapter_name: lora_dropout_layer}))\n","\n","        # Actual trainable parameters\n","        # EXERCISE: write a linear layer that goes from self.in_features to r\n","        # and without bias\n","        self.lora_A[adapter_name] =\n","        # EXERCISE: write a linear layer that goes from r to self.out_features\n","        # and without bias\n","        self.lora_B[adapter_name] =\n","\n","        self.scaling[adapter_name] = lora_alpha / r\n","\n","        self.reset_lora_parameters(adapter_name, init_lora_weights)\n","        self.set_adapter(self.active_adapters)\n","\n","    def forward(self, x, *args, **kwargs):\n","        result = self.base_layer(x, *args, **kwargs)\n","        torch_result_dtype = result.dtype\n","        for active_adapter in self.active_adapters:\n","            if active_adapter not in self.lora_A.keys():\n","                continue\n","            lora_A = self.lora_A[active_adapter]\n","            lora_B = self.lora_B[active_adapter]\n","            dropout = self.lora_dropout[active_adapter]\n","            scaling = self.scaling[active_adapter]\n","\n","            x = x.to(lora_A.weight.dtype)\n","\n","            x = dropout(x)\n","\n","            # EXERCISE: add to the result of the base layer, the output of\n","            # lora_B and lora_A and multiply by the scaling factor\n","            result = result +\n","\n","        result = result.to(torch_result_dtype)\n","\n","        return result"],"metadata":{"id":"e9BeyUqp7UGm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Since we are using the HuggingFace PEFT library framework, we need to tweak some of its internal workings to be able to expose the LoRA layer above. Therefore the following cell is not very insightful to understand."],"metadata":{"id":"4h2IO1wO7g9j"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JRP7Mq0L8mD8"},"outputs":[],"source":["def custom_dispatch_default(target: torch.nn.Module, adapter_name, lora_config, **kwargs):\n","    new_module = None\n","    target_base_layer = target.get_base_layer() if isinstance(target, BaseTunerLayer) else target\n","\n","    if isinstance(target_base_layer, torch.nn.Linear):\n","        kwargs.update(lora_config.loftq_config)\n","        new_module = CustomLinearLoRA(target, adapter_name, **kwargs)\n","\n","    if new_module is None:\n","        new_module = dispatch_default(target, adapter_name, lora_config=lora_config, **kwargs)\n","    return new_module\n","\n","class CustomLoraModel(LoraModel):\n","    @staticmethod\n","    def _create_new_module(lora_config, adapter_name, target, **kwargs):\n","        return custom_dispatch_default(target, adapter_name, lora_config=lora_config, **kwargs)\n","\n","class CustomPeftModel(PeftModel):\n","    def __init__(self, model, peft_config, adapter_name=\"default\"):\n","        PushToHubMixin.__init__(self)\n","        torch.nn.Module.__init__(self)\n","\n","        self.modules_to_save = None\n","        self.active_adapter = adapter_name\n","        self.peft_type = peft_config.peft_type\n","        # These args are special PEFT arguments that users can pass. They need to be removed before passing them to\n","        # forward.\n","        self.special_peft_forward_args = {\"adapter_names\"}\n","\n","        self._is_prompt_learning = peft_config.is_prompt_learning\n","        self._peft_config = None\n","        self.base_model = CustomLoraModel(model, {adapter_name: peft_config}, adapter_name)\n","\n","        self.set_additional_trainable_modules(peft_config, adapter_name)\n","\n","        if getattr(model, \"is_gradient_checkpointing\", True):\n","            model = self._prepare_model_for_gradient_checkpointing(model)\n","\n","        # the `pretraining_tp` is set for some models to simulate Tensor Parallelism during inference to avoid\n","        # numerical differences, https://github.com/pytorch/pytorch/issues/76232 - to avoid any unexpected\n","        # behavior we disable that in this line.\n","        if hasattr(self.base_model, \"config\") and hasattr(self.base_model.config, \"pretraining_tp\"):\n","            self.base_model.config.pretraining_tp = 1"]},{"cell_type":"markdown","source":["Now we have everything we need to fine-tune BERT with LoRA. We load again the model, we upgrade it with LoRA, we count the trainable parameters and let's see what happens when we fine-tune it."],"metadata":{"id":"ojIzZpQl8Y3s"}},{"cell_type":"code","source":["model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, num_labels=5)\n","\n","model = CustomPeftModel(model, peft_config)\n","print_trainable_parameters(model)"],"metadata":{"id":"7XnFXXab7q7J"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xxe5mXn3yiOK"},"outputs":[],"source":["# EXERCISE: explore learning rates in the set [5e-2, 5e-3, 5e-4, 5e-5] to find the best\n","# one with this configuration\n","train_and_evaluate(model, learning_rate="]},{"cell_type":"markdown","source":["As you see, LoRA was faster than full fine-tuning, with a better final performance than just updating the last linear layer."],"metadata":{"id":"aF4F0PxD7v8Q"}},{"cell_type":"markdown","source":["## âœ¨ Resources used for this tutorial and references\n","- [LORA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685)\n","- [DoRA: Weight-Decomposed Low-Rank Adaptation](https://arxiv.org/pdf/2402.09353)\n","- [HuggingFace PEFT Tutorial](https://huggingface.co/blog/peft)\n","- [HuggingFace PEFT Tutorial for image classification](https://huggingface.co/docs/peft/main/en/task_guides/image_classification_lora)\n","- [HuggingFace Training Tutorial](https://huggingface.co/docs/transformers/training)\n"],"metadata":{"id":"5B2EVpV4_8q6"}}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}