{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","machine_shape":"hm","toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Natural Language Processing Tutorial\n","======\n","\n","This is the tutorial of the 2024 [Mediterranean Machine Learning Summer School](https://www.m2lschool.org/) on Natural Language Processing!\n","\n","This tutorial will explore the fundamental aspects of Natural Language Processing (NLP). Basic Python programming skills are expected.\n","Prior knowledge of standard NLP techniques (e.g. text tokenization and classification with ML) is beneficial but optional when working through the notebooks as they assume minimal prior knowledge.\n","\n","This tutorial combines detailed analysis and development of essential NLP concepts via custom (i.e. from scratch) implementations. Other necessary NLP components will be developed using PyTorch's NLP library implementations. As a result, the tutorial offers deep understanding and facilitates easy usage in future applications.\n","\n","## Outline\n","\n","* Part I: Introduction to Text Tokenization and Classification\n","  *  Text Classification: Simple Classifier\n","  *  Text Classification: Encoder-only Transformer\n","\n","* Part II: Introduction to Decoder-only Transformer and Sparse Mixture of Experts Architecture\n","  *  Text Generation: Decoder-only Transformer\n","  *  Text Generation: Decoder-only Transformer + MoE\n","\n","* Part III: Introduction to Parameter Efficient Fine-tuning\n","  *  Fine-tuning the full Pre-trained Models\n","  *  Fine-tuning using Low-Rank Adaptation of Large Language Models (LoRA)\n","\n","## Notation\n","\n","* Sections marked as [📝] contain cells with missing code that you should complete.\n","* Sections marked with [📚] contain cells that you should read and modify to understand how your changes alter the obtained results.\n","* External resources are mentioned with [✨]. These provide valuable supplementary information for this tutorial and offer opportunities for further in-depth exploration of the topics covered.\n","* Sections that contain code that test the functionality of other sections are marked with [✍]. You are more that welcome to modify these sections so that you can understand code functionality.\n","\n","\n","## Libraries\n","\n","This tutorial leverages [PyTorch](https://pytorch.org/) for neural network implementation and training, complemented by standard Python libraries for data processing and the [Hugging Face](https://huggingface.co/) datasets library for accessing NLP resources.\n","\n","GPU access is recommended for optimal performance, particularly for model training and text generation. While all code can run on CPU, a CUDA-enabled environment will significantly speed up these processes.\n","\n","## Credits\n","\n","The tutorial is created by:\n","\n","* [Georgios Peikos](https://www.linkedin.com/in/peikosgeorgios/)\n","* [Luca Herranz-Celotti](http://LuCeHe.github.io)\n","\n","It is inspired by and synthesizes various online resources, which are cited throughout for reference and further reading.\n","\n","## Note for Colab users\n","\n","To grab a GPU (if available), make sure you go to `Edit -> Notebook settings` and choose a GPU under `Hardware accelerator`\n","\n"],"metadata":{"id":"KBrDjSR61FHy"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"nhQDxGSyOQh3"}},{"cell_type":"markdown","source":["# Part II: Introduction to the decoder-only Transformers architecture and Sparse Mixture of Expert\n","\n","We create a decoder-only Transformer architecture from the bottom up, including a custom text tokenizer and an efficient dataset handler. We will explore all essential components of this architecture, train the model, and show its capabilities in text generation.\n","\n","Then, we will enhance our base model by incorporating a gating function and implementing a sparse mixture of experts."],"metadata":{"id":"cCNKdaEAOFeJ"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"7wFJ2UqTOcpT"}},{"cell_type":"markdown","source":["# Decoder-only Transformer Architecture\n","\n","The decoder-only transformer architecture consists of multiple identical blocks stacked sequentially. Each block is composed of two main elements:\n","- A masked multi-head self-attention mechanism.\n","- A feed-forward neural network.\n","\n","These components are typically encapsulated within residual connections and layer normalization. In this section, we will explore the internal structure of these blocks in greater depth and provide a practical PyTorch implementation."],"metadata":{"id":"bI8usToL1SAM"}},{"cell_type":"markdown","source":["![Decoder Only Architecture](https://drive.google.com/uc?id=1ksROxQxf3b7dlBUoIQggzyLeBaPO-AQn)\n","\n","\n"],"metadata":{"id":"V1Z2GWVTShRR"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"UJPlhQquMr9W"}},{"cell_type":"markdown","source":["## Importing Libraries"],"metadata":{"id":"cSQDU10O6YUC"}},{"cell_type":"code","source":["!pip install datasets\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import math\n","from collections import Counter\n","from typing import List, Tuple, Union\n","from datasets import load_dataset\n","import torch.nn.functional as F\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"],"metadata":{"id":"E532K_c76aIB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 📚 Text Tokenization from Scratch\n","\n","Tokenization is a fundamental step in NLP that converts raw text into a format that systems can understand and process. It enables the transformation of variable-length text sequences into fixed-size numerical representations, which is crucial for input to neural network models.\n","\n","Here, we create a simple text tokenizer for basic word-level tokenization tasks.\n","The tokenizer can be improved so that:\n","\n","1.   Methods for handling very large vocabularies (e.g., frequency thresholding)\n","2.   Support for n-grams or phrase detection\n","3.   Handle punctuation. For instance now, tokens like \"word.\" and \"word\" are being treated differently.\n","\n","Also, we create a testing function to showcase the codes behavior.\n","\n","**✨ Additional Resources:**\n","\n","*   Overview of hugging Face tokenizers [Link-huggingface](https://huggingface.co/docs/transformers/en/tokenizer_summary)"],"metadata":{"id":"HNcnHkLd6Vab"}},{"cell_type":"code","source":["class SimpleTokenizer:\n","    def __init__(self):\n","        \"\"\"Initialize the tokenizer with special tokens and prepare vocabulary structures.\"\"\"\n","        # Special tokens are used for various purposes in NLP tasks:\n","        # <PAD>: Used for padding sequences to a fixed length\n","        # <UNK>: Represents unknown words not in the vocabulary\n","        # <SOS>: Marks the start of a sequence\n","        # <EOS>: Marks the end of a sequence\n","        self.special_tokens = [\"<PAD>\", \"<UNK>\", \"<SOS>\", \"<EOS>\"]\n","\n","        # word_to_idx: Maps words to unique integer indices\n","        # This is crucial for converting text into a format that neural networks can process\n","        self.word_to_idx = {token: idx for idx, token in enumerate(self.special_tokens)}\n","\n","        # idx_to_word: The reverse mapping of word_to_idx\n","        # This is used for converting model outputs back into readable text\n","        self.idx_to_word = {idx: token for idx, token in enumerate(self.special_tokens)}\n","\n","        # Counter object to keep track of word frequencies in the corpus\n","        self.word_count = Counter()\n","\n","    def fit(self, texts: List[str]) -> None:\n","        \"\"\"Build the vocabulary from a list of texts.\"\"\"\n","        # Count the frequency of each word in the entire corpus\n","        for text in texts:\n","            self.word_count.update(text.split())\n","\n","        # Add each unique word to the vocabulary\n","        # We assign a unique index to each word, which the model will use to represent words\n","        for word in self.word_count:\n","            if word not in self.word_to_idx:\n","                idx = len(self.word_to_idx)\n","                self.word_to_idx[word] = idx\n","                self.idx_to_word[idx] = word\n","\n","    def encode(self, text: str) -> List[int]:\n","        \"\"\"Convert a text string to a list of indices.\"\"\"\n","        # This method is used to prepare input for the model\n","        # It converts each word to its corresponding index\n","        # If a word is not in the vocabulary, it uses the <UNK> token\n","        return [self.word_to_idx.get(word, self.word_to_idx[\"<UNK>\"]) for word in text.split()]\n","\n","    def decode(self, indices: List[int]) -> str:\n","        \"\"\"Convert a list of indices back to a text string.\"\"\"\n","        # This method is used to convert model output back into readable text\n","        # It maps each index back to its corresponding word\n","        return \" \".join([self.idx_to_word.get(idx, \"<UNK>\") for idx in indices])\n","\n","    def encode_batch(self, texts: List[str]) -> List[List[int]]:\n","        \"\"\"Convert a batch of text strings to lists of indices.\"\"\"\n","        return [self.encode(text) for text in texts]\n","\n","    def decode_batch(self, batch_indices: List[List[int]]) -> List[str]:\n","        \"\"\"Convert a batch of lists of indices back to text strings.\"\"\"\n","        return [self.decode(indices) for indices in batch_indices]\n","\n","    def show_vocab(self):\n","        \"\"\"Display the vocabulary.\"\"\"\n","        # Useful for debugging and understanding the tokenizer's state\n","        print(\"Vocabulary:\")\n","        for word, idx in self.word_to_idx.items():\n","            print(f\"{word}: {idx}\")\n","\n","    def __len__(self):\n","        \"\"\"Return the size of the vocabulary.\"\"\"\n","        # The vocabulary size is an important parameter for the model\n","        # It determines the dimensionality of the model's output layer\n","        return len(self.word_to_idx)"],"metadata":{"id":"mZ6q24hC1Q2J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ✍ Testing the Tokenizer\n","\n","This testing function shows examples of text tokenization presenting also extreme use cases."],"metadata":{"id":"V8FKgjcjZyj2"}},{"cell_type":"code","source":["def test_tokenizer():\n","    print(\"\\nTesting SimpleTokenizer\")\n","    print(\"=\" * 30)\n","\n","    # Sample texts\n","    texts = [\n","        \"The quick brown fox jumps over the lazy dog.\",\n","        \"Pack my box with five dozen liquor jugs!\",\n","        \"How vexingly quick daft zebras jump!\",\n","        \"This is a sentence with some punctuation, including commas.\",\n","        \"This text contains an unknown word: monkey\",\n","        \"\"  # Empty string to test edge case\n","    ]\n","\n","    # Initialize and fit the tokenizer\n","    tokenizer = SimpleTokenizer()\n","    tokenizer.fit(texts)\n","\n","    # Display vocabulary\n","    tokenizer.show_vocab()\n","    print(f\"\\nVocabulary size: {len(tokenizer)}\")\n","\n","    # Test encoding and decoding\n","    print(\"\\nEncoding and Decoding Test:\")\n","    for text in texts:\n","        encoded = tokenizer.encode(text)\n","        decoded = tokenizer.decode(encoded)\n","        print(f\"\\nOriginal: {text}\")\n","        print(f\"Encoded : {encoded}\")\n","        print(f\"Decoded : {decoded}\")\n","        print(f\"Match   : {'✓' if text.strip().lower() == decoded.strip().lower() else '✗'}\")\n","\n","    # Test unknown word handling\n","    print(\"\\nUnknown Word Handling Test:\")\n","    unknown_text = \"This text contains an unknown word: xylophone\"\n","    encoded_unknown = tokenizer.encode(unknown_text)\n","    decoded_unknown = tokenizer.decode(encoded_unknown)\n","    print(f\"Original: {unknown_text}\")\n","    print(f\"Encoded : {encoded_unknown}\")\n","    print(f\"Decoded : {decoded_unknown}\")\n","\n","    # Test special tokens\n","    print(\"\\nSpecial Tokens Test:\")\n","    special_text = \"< SOS > This is a test sentence <EOS>\"\n","    encoded_special = tokenizer.encode(special_text)\n","    decoded_special = tokenizer.decode(encoded_special)\n","    print(f\"Original: {special_text}\")\n","    print(f\"Encoded : {encoded_special}\")\n","    print(f\"Decoded : {decoded_special}\")\n","\n","    # Test case sensitivity\n","    print(\"\\nCase Sensitivity Test:\")\n","    case_text = \"The Quick Brown Fox\"\n","    encoded_case = tokenizer.encode(case_text)\n","    decoded_case = tokenizer.decode(encoded_case)\n","    print(f\"Original: {case_text}\")\n","    print(f\"Encoded : {encoded_case}\")\n","    print(f\"Decoded : {decoded_case}\")\n","\n","print(\"\\nChecking the tokenizer's outputs\")\n","test_tokenizer()"],"metadata":{"collapsed":true,"id":"tc3YteacXJOs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 📚 TextDataset: Efficient Text Processing\n","\n","The TextDataset class is a crucial component in preparing text data for deep learning models, implementing a sliding window approach that allows processing of variable-length texts while maintaining context.\n","\n","This class bridges the gap between raw text data and the input requirements of neural networks, handling tasks such as tokenization, padding, and attention mask generation, which are essential for training effective sequence models like Transformers.\n","\n","**✨ Additional Resources:**\n","\n","*   Padding and truncation [Link-huggingface](https://huggingface.co/docs/transformers/en/pad_truncation)\n"],"metadata":{"id":"JS3le4cC6do5"}},{"cell_type":"code","source":["class TextDataset(Dataset):\n","    def __init__(self, texts: List[str], tokenizer: SimpleTokenizer, max_length: int, overlap: int = 50):\n","        \"\"\"\n","        Initialize the TextDataset with sliding window functionality.\n","\n","        Args:\n","            texts (List[str]): List of input texts.\n","            tokenizer (SimpleTokenizer): Tokenizer object for encoding texts.\n","            max_length (int): Maximum length of encoded sequences.\n","            overlap (int): Number of overlapping tokens between windows.\n","        \"\"\"\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","        self.overlap = overlap\n","        self.data = []\n","        self.attention_masks = []\n","        self.document_map = []  # Maps each window to its original document\n","        self.original_texts = texts  # Store original texts\n","\n","        for doc_idx, text in enumerate(texts):\n","            tokens = self.tokenizer.encode(text)\n","            windows = self.create_sliding_windows(tokens)\n","\n","            for window in windows:\n","                attention_mask = [1] * len(window)  # 1 for real tokens\n","\n","                # Pad if necessary\n","                if len(window) < max_length:\n","                    padding_length = max_length - len(window)\n","                    window = window + [self.tokenizer.word_to_idx[\"<PAD>\"]] * padding_length\n","                    attention_mask = attention_mask + [0] * padding_length  # 0 for padding in attention mask\n","\n","                self.data.append(window)\n","                self.attention_masks.append(attention_mask)\n","                self.document_map.append(doc_idx)\n","\n","    def create_sliding_windows(self, tokens: List[int]) -> List[List[int]]:\n","        \"\"\"\n","        Create sliding windows from a list of tokens.\n","\n","        Args:\n","            tokens (List[int]): List of token ids.\n","\n","        Returns:\n","            List[List[int]]: List of token windows.\n","        \"\"\"\n","        windows = []\n","        # Calculate stride: how many tokens to move for each new window\n","        # -1 accounts for the added <SOS> token at the start of each window\n","        stride = self.max_length - self.overlap - 1\n","\n","        for start in range(0, len(tokens), stride):\n","            # Create a window starting with <SOS> token\n","            window = [self.tokenizer.word_to_idx[\"<SOS>\"]] + tokens[start:start + self.max_length - 1]\n","            if len(window) < self.max_length:\n","                # This is the last window, add <EOS> token\n","                window.append(self.tokenizer.word_to_idx[\"<EOS>\"])\n","            windows.append(window)\n","\n","        return windows\n","\n","    def get_original_document(self, doc_idx: int) -> str:\n","        \"\"\"Retrieve the original document text.\"\"\"\n","        if 0 <= doc_idx < len(self.original_texts):\n","            return self.original_texts[doc_idx]\n","        else:\n","            raise IndexError(f\"Document index {doc_idx} is out of range.\")\n","\n","    def get_document_length(self, doc_idx: int) -> int:\n","        \"\"\"Get the number of tokens in the original document.\"\"\"\n","        if 0 <= doc_idx < len(self.original_texts):\n","            return len(self.tokenizer.encode(self.original_texts[doc_idx]))\n","        else:\n","            raise IndexError(f\"Document index {doc_idx} is out of range.\")\n","\n","    def window_to_document_position(self, window_idx: int, token_idx: int) -> Tuple[int, int]:\n","        \"\"\"Map a position in a window back to its position in the original document.\"\"\"\n","        if 0 <= window_idx < len(self.data):\n","            doc_idx = self.document_map[window_idx]\n","            doc_windows = self.get_document_windows(doc_idx)\n","            # Find which window of the document this is\n","            relative_window_idx = doc_windows.index(window_idx)\n","            # Calculate the start position of this window in the document\n","            window_start = relative_window_idx * (self.max_length - self.overlap - 1)\n","            # -1 to account for <SOS> token at the start of each window\n","            return doc_idx, window_start + token_idx - 1\n","        else:\n","            raise IndexError(f\"Window index {window_idx} is out of range.\")\n","\n","    def __len__(self) -> int:\n","        \"\"\"Get the number of windows in the dataset.\"\"\"\n","        return len(self.data)\n","\n","    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, int]:\n","        \"\"\"\n","        Get a sample from the dataset.\n","\n","        Args:\n","            idx (int): Index of the sample.\n","\n","        Returns:\n","            Tuple[torch.Tensor, torch.Tensor, int]:\n","                A tuple containing (token_ids, attention_mask, document_index).\n","        \"\"\"\n","        if 0 <= idx < len(self.data):\n","            # Add an extra dimension to make it batch-first (batch_size=1)\n","            return (torch.tensor(self.data[idx]).unsqueeze(0),\n","                    torch.tensor(self.attention_masks[idx]).unsqueeze(0),\n","                    self.document_map[idx])\n","        else:\n","            raise IndexError(f\"Index {idx} is out of range.\")\n","\n","\n","    def get_document_windows(self, doc_idx: int) -> List[int]:\n","        \"\"\"\n","        Get all window indices for a specific document.\n","\n","        Args:\n","            doc_idx (int): Index of the document.\n","\n","        Returns:\n","            List[int]: List of window indices belonging to the document.\n","        \"\"\"\n","        return [i for i, doc in enumerate(self.document_map) if doc == doc_idx]"],"metadata":{"id":"CeBCUMY16jms"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ✍ Testing the Dataset Processing\n","\n","This testing function shows how the TextDataset and SimpleTokenizer classes work together."],"metadata":{"id":"QMa3_yaQgAYf"}},{"cell_type":"code","source":["def test_sliding_window_dataset():\n","    print(\"\\n--- Testing Sliding Window Dataset ---\\n\")\n","\n","    texts = [\n","        \"This is a short sentence.\",\n","        \"This is a much longer sentence that will be split into multiple windows to demonstrate the sliding window approach. It contains enough tokens to create at least two or three windows depending on the chosen maximum length and overlap.\",\n","        \"Another sentence of medium length that might create two windows.\",\n","        \"\",  # Empty text to test edge case\n","        \"Short.\"  # Very short text to test edge case\n","    ]\n","\n","    try:\n","        tokenizer = SimpleTokenizer()\n","        tokenizer.fit(texts)\n","\n","        max_length = 16\n","        overlap = 5\n","        dataset = TextDataset(texts, tokenizer, max_length, overlap)\n","\n","        print(f\"Dataset configuration:\")\n","        print(f\"  Max length: {max_length}\")\n","        print(f\"  Overlap: {overlap}\")\n","        print(f\"  Total windows: {len(dataset)}\")\n","        print(f\"  Vocabulary size: {len(tokenizer)}\\n\")\n","\n","        for doc_idx, text in enumerate(texts):\n","            print(f\"Document {doc_idx}:\")\n","            print(f\"  Original text: '{text}'\")\n","            print(f\"  Original length: {len(text.split())}\")\n","\n","            window_indices = dataset.get_document_windows(doc_idx)\n","            print(f\"  Number of windows: {len(window_indices)}\")\n","\n","            for i, window_idx in enumerate(window_indices):\n","                tokens, attention_mask, _ = dataset[window_idx]\n","                # Remove the batch dimension for decoding\n","                decoded = tokenizer.decode(tokens.squeeze(0).tolist())\n","                print(f\"\\n    Window {i}:\")\n","                print(f\"    Tokens shape: {tokens.shape}\")\n","                print(f\"    Tokens: {tokens.squeeze(0).tolist()}\")\n","                print(f\"    Attention mask shape: {attention_mask.shape}\")\n","                print(f\"    Attention mask: {attention_mask.squeeze(0).tolist()}\")\n","                print(f\"    Decoded: '{decoded}'\")\n","                print(f\"    Window length: {tokens.size(1)}\")  # Use size(1) for sequence length\n","\n","            print(\"\\n\" + \"-\"*50)\n","\n","        tokenizer.show_vocab()\n","\n","    except Exception as e:\n","        print(f\"An error occurred: {str(e)}\")\n","\n","    print(\"\\n--- End of Test ---\")\n","\n","# Run the test\n","test_sliding_window_dataset()"],"metadata":{"collapsed":true,"id":"AwEtTxasgHC4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 📝 Positional Encoding\n","\n","Positional Encoding adds information about the position of each token in the sequence. This is necessary because the self-attention mechanism in Transformers doesn't inherently have a notion of token order.\n","\n","\n","\\begin{equation}\n","PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)\n","\\end{equation}\n","\n","\\begin{equation}\n","PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)\n","\\end{equation}\n","\n","\n","\n","**✨ Additional Resources:**\n","\n","*   Transformer Architecture: The Positional Encoding [Link-kazemnejad](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)\n","\n","*   Positional Encoding in Transformers [Link-geeksforgeeks](https://www.geeksforgeeks.org/positional-encoding-in-transformers/)\n","\n","\n"],"metadata":{"id":"BHKvbddC6lpB"}},{"cell_type":"code","source":["class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, d_model, max_len=5000):\n","        \"\"\"\n","        Inputs\n","            d_model - Hidden dimensionality of the input.\n","            max_len - Maximum length of a sequence to expect.\n","        \"\"\"\n","        super().__init__()\n","\n","        #######################Code Here###############################\n","\n","        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n","        ## pe = ...\n","\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","\n","        # Compute the `div_term` for the frequency component using the formula involving d_model\n","        ## div_term =\n","\n","\n","        # Populate the even indices (0, 2, 4, ...) of `pe` with the sine of the position multiplied by the `div_term`\n","        ## pe[:, 0::2] =\n","\n","\n","        # Populate the odd indices (1, 3, 5, ...) of `pe` with the cosine of the position multiplied by the `div_term`\n","\n","\n","        # Add an *extra dimension* to `pe` to fit the batch dimension requirements\n","\n","\n","        # Register `pe` as a buffer to ensure it is not considered a model parameter but is still moved to the appropriate device when the model is moved\n","\n","\n","        ###############################################################\n","\n","    def forward(self, x):\n","        x = x + self.pe[:, :x.size(1)]\n","        return x"],"metadata":{"id":"9owPpdm-neKA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ✍ Testing your Positional Encoding Function"],"metadata":{"id":"mLYaklgz6bq5"}},{"cell_type":"code","source":["def test_positional_encoding_consistency():\n","    # Test parameters\n","    d_model = 4\n","    max_len = 10\n","    batch_size = 2\n","    seq_len = 3\n","\n","    # Initialize a new PositionalEncoding module each time\n","    pe = PositionalEncoding(d_model, max_len)\n","\n","    # Create a small input tensor\n","    x1 = torch.zeros(batch_size, seq_len, d_model)\n","\n","    # Run the positional encoding\n","    output = pe(x1)\n","\n","    print(\"Input:\")\n","    print(x1)\n","    print(\"\\nOutput:\")\n","    print(output)\n","\n","    print(\"\"\"\\nNote: If your\n","    Output:\n","        tensor([[[ 0.0000,  1.0000,  0.0000,  1.0000],\n","                [ 0.8415,  0.5403,  0.0100,  0.9999],\n","                [ 0.9093, -0.4161,  0.0200,  0.9998]],\n","\n","                [[ 0.0000,  1.0000,  0.0000,  1.0000],\n","                [ 0.8415,  0.5403,  0.0100,  0.9999],\n","                [ 0.9093, -0.4161,  0.0200,  0.9998]]])\n","             , the PositionalEncoding is consistent.\"\"\")\n","\n","test_positional_encoding_consistency()"],"metadata":{"id":"LfSwCfMy6gZH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ✍ Testing the Positional Encoding for Text"],"metadata":{"id":"vcOi9vZdmc1t"}},{"cell_type":"code","source":["def test_positional_encoding_with_dataset():\n","    print(\"\\n--- Testing Positional Encoding with Dataset ---\\n\")\n","\n","    # Set a fixed seed for reproducibility\n","    torch.manual_seed(42)\n","\n","    # Sample texts\n","    texts = [\n","        \"This is a short sentence.\",\n","        \"This is a much longer sentence that will be split into multiple windows. Observe the term overlapping?\",\n","        \"Another sentence of medium length.\"\n","    ]\n","\n","    # Initialize tokenizer and fit it to the texts\n","    tokenizer = SimpleTokenizer()\n","    tokenizer.fit(texts)\n","\n","    # Create dataset\n","    max_length = 10\n","    overlap = 2\n","    dataset = TextDataset(texts, tokenizer, max_length, overlap)\n","\n","    # Initialize positional encoding\n","    d_model = 16  # Small dimension for demonstration\n","    pos_encoder = PositionalEncoding(d_model, max_length)\n","\n","    print(f\"Dataset configuration:\")\n","    print(f\"  Max length: {max_length}\")\n","    print(f\"  Overlap: {overlap}\")\n","    print(f\"  Total windows: {len(dataset)}\")\n","    print(f\"  Vocabulary size: {len(tokenizer)}\")\n","    print(f\"  Embedding dimension: {d_model}\\n\")\n","\n","    # Process each window through the positional encoding\n","    for i in range(len(dataset)):\n","        tokens, attention_mask, doc_idx = dataset[i]\n","\n","        # Convert tokens to \"embeddings\" (just for demonstration)\n","        pseudo_embeddings = torch.rand(1, tokens.size(1), d_model)  # (batch_size, seq_len, d_model)\n","\n","        # Apply positional encoding\n","        encoded = pos_encoder(pseudo_embeddings)\n","\n","        print(f\"Window {i} (from document {doc_idx}):\")\n","        print(f\"  Original tokens: {tokens.squeeze(0).tolist()}\")\n","        print(f\"  Attention mask: {attention_mask.squeeze(0).tolist()}\")\n","        print(f\"  Decoded: '{tokenizer.decode(tokens.squeeze(0).tolist())}'\")\n","        print(f\"  Shape after positional encoding: {encoded.shape}\")\n","\n","        # Display the positional encoding effect for all tokens\n","        print(f\"  Positional encoding effect:\")\n","        for j in range(tokens.size(1)):\n","            if attention_mask[0, j] == 1:  # Only show for non-padding tokens\n","                print(f\"    Token {j}:\")\n","                print(f\"      Before: {pseudo_embeddings[0, j, :].tolist()}\")\n","                print(f\"      After:  {encoded[0, j, :].tolist()}\")\n","\n","        print()\n","\n","    print(\"--- End of Test ---\")\n","\n","# Run the test\n","test_positional_encoding_with_dataset()"],"metadata":{"id":"jOzH22PBmfyG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 📝 Masked Multihead Attention Mechanism\n","\n","As you have implemented the Attention Mechanism in Part I, here you will have to use its ready implementation from Pytorch.\n","\n","Masked Attention mechanism allows the transformer model to focus on relevant parts of the input sequence while preventing information leakage from future tokens during sequential processing (i.e. we use the term Masked).\n","\n","Please, visit [Link-pytorch](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html) for a detailed description of the MultiheadAttention function in PyTorch.\n","\n","**✨ Additional Resources:**\n","\n","*   Multi-head Attention, deep dive [Link-towardsdatascience](https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853)\n","* Attention Is All You Need (original Transformer paper) [Link-ArXiv](https://arxiv.org/abs/1706.03762)\n","\n","* A visual explanation of the attention mechanism [Link-youtube](https://www.youtube.com/watch?v=bCz4OMemCcA&t=1208s&ab_channel=UmarJamil)"],"metadata":{"id":"i_d5KqeSAdiv"}},{"cell_type":"code","source":["class MaskedAttention(nn.Module):\n","    def __init__(self, d_model: int, nhead: int, dropout: float = 0.1):\n","        super().__init__()\n","\n","        #######################Code Here###############################\n","\n","        # Initialize the MultiheadAttention layer using its PyTorch implementation\n","\n","        # Initialize the parameters\n","\n","        ###############################################################\n","\n","\n","    def generate_square_subsequent_mask(self, sz: int) -> torch.Tensor:\n","\n","        #######################Code Here###############################\n","        # Create a triangular mask tensor in PyTorch where the lower triangle (including the diagonal) is\n","        # filled with ones and the upper triangle is filled with zeros. The tensor should be of size (sz, sz),\n","        # where sz is a given integer.\n","        ##############################################################\n","\n","        # The masked positions are filled with float('-inf'). Unmasked positions are filled with float(0.0). See: https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#Transformer\n","        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","        return mask\n","\n","    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n","        seq_len = x.size(1)\n","        attn_mask = self.generate_square_subsequent_mask(seq_len).to(x.device)\n","        output, _ = self.multihead_attn(x, x, x, attn_mask=attn_mask)\n","        return output, attn_mask"],"metadata":{"id":"oDqt8XggAlFe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ✍ Testing the Multihead Attention"],"metadata":{"id":"MAMRpHUKwGKP"}},{"cell_type":"code","source":["def test_expanded_transformer_components():\n","    print(\"\\n--- Testing Expanded Transformer Components ---\\n\")\n","\n","    # Set a fixed seed for reproducibility\n","    torch.manual_seed(42)\n","\n","    # Sample texts\n","    texts = [\n","        \"This is a short sentence.\",\n","        \"This is a much longer sentence that will be split into multiple windows. Observe the term overlapping?\",\n","        \"Another sentence of medium length.\"\n","    ]\n","\n","    # Initialize tokenizer and fit it to the texts\n","    tokenizer = SimpleTokenizer()\n","    tokenizer.fit(texts)\n","\n","    # Create dataset\n","    max_length = 10\n","    overlap = 2\n","    dataset = TextDataset(texts, tokenizer, max_length, overlap)\n","\n","    # Hyperparameters\n","    d_model = 16  # Small dimension for demonstration\n","    nhead = 2\n","\n","    # Initialize components\n","    pos_encoder = PositionalEncoding(d_model, max_length)\n","    masked_self_attn = MaskedAttention(d_model, nhead)\n","\n","    print(f\"Dataset configuration:\")\n","    print(f\"  Max length: {max_length}\")\n","    print(f\"  Overlap: {overlap}\")\n","    print(f\"  Total windows: {len(dataset)}\")\n","    print(f\"  Vocabulary size: {len(tokenizer)}\")\n","    print(f\"  Embedding dimension: {d_model}\")\n","    print(f\"  Number of attention heads: {nhead}\\n\")\n","\n","    # Process each window\n","    for i in range(len(dataset)):\n","        tokens, attention_mask, doc_idx = dataset[i]\n","\n","        print(f\"Window {i} (from document {doc_idx}):\")\n","        print(f\"  Original tokens: {tokens.squeeze(0).tolist()}\")\n","        print(f\"  Attention mask: {attention_mask.squeeze(0).tolist()}\")\n","        print(f\"  Decoded: '{tokenizer.decode(tokens.squeeze(0).tolist())}'\")\n","\n","        # Convert tokens to \"embeddings\" (just for demonstration)\n","        pseudo_embeddings = torch.rand(1, tokens.size(1), d_model)  # (batch_size, seq_len, d_model)\n","        print(f\"  Shape of pseudo embeddings: {pseudo_embeddings.shape}\")\n","\n","        # Apply positional encoding\n","        pos_encoded = pos_encoder(pseudo_embeddings)\n","        print(f\"  Shape after positional encoding: {pos_encoded.shape}\")\n","\n","        # Apply masked self-attention\n","        attn_output, attn_mask = masked_self_attn(pos_encoded)\n","        print(f\"  Shape after masked self-attention: {attn_output.shape}\")\n","\n","        # Display the effect of positional encoding and attention for all tokens\n","        print(f\"  Transformer effect on tokens:\")\n","        for j in range(tokens.size(1)):\n","            if attention_mask[0, j] == 1:  # Only show for non-padding tokens\n","                print(f\"    Token {j}:\")\n","                print(f\"      Initial:   {pseudo_embeddings[0, j, :5].tolist()}\")\n","                print(f\"      Positional:{pos_encoded[0, j, :5].tolist()}\")\n","                print(f\"      Attention Mask: {attn_mask[j, :5].tolist()}\")  # Show first 5 values of attention mask\n","                print(f\"      Attention: {attn_output[0, j, :5].tolist()}\")\n","        print()\n","\n","    print(\"--- End of Expanded Test ---\")\n","\n","# Run the expanded test\n","test_expanded_transformer_components()"],"metadata":{"id":"e0UqHQVswJzX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 📝 Feed Forward Netwrok\n","\n","A feed-forward network is a multi-layered structure in which information moves in a single direction, from the input layer to the output layer.\n","\n","\n","**✨ Additional Resources:**\n","\n","*   Transformer Feed-Forward Layers Are Key-Value Memories\n"," [Link-ArXiv](https://arxiv.org/abs/2012.14913)\n","\n","\n","\n","\n","\n"],"metadata":{"id":"AGmlNshUAmmM"}},{"cell_type":"code","source":["class FeedForward(nn.Module):\n","    def __init__(self, d_model, d_ff, dropout=0.1):\n","        super().__init__()\n","\n","        #######################Code Here###############################\n","        # Define feed-forward network using nn.Sequential\n","        self.net = nn.Sequential(\n","          # First linear layer\n","          # ReLU activation\n","          # Dropout for regularization\n","          # Second linear layer\n","        )\n","        ###############################################################\n","\n","\n","    def forward(self, x):\n","        return self.net(x)  # Apply the feed-forward network"],"metadata":{"id":"yH4rocZtFSY7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ✍ Testing the Feed Forward Layer"],"metadata":{"id":"XxPG41aLFJz6"}},{"cell_type":"code","source":["def test_all_transformer_components():\n","    print(\"\\n--- Testing Transformer Components ---\\n\")\n","\n","    # Set a fixed seed for reproducibility\n","    torch.manual_seed(42)\n","\n","    # Sample texts\n","    texts = [\n","        \"This is a short sentence.\",\n","        \"This is a much longer sentence that will be split into multiple windows. Observe the term overlapping?\",\n","        \"Another sentence of medium length.\"\n","    ]\n","\n","    # Initialize tokenizer and fit it to the texts\n","    tokenizer = SimpleTokenizer()\n","    tokenizer.fit(texts)\n","\n","    # Create dataset\n","    max_length = 10\n","    overlap = 2\n","    dataset = TextDataset(texts, tokenizer, max_length, overlap)\n","\n","    # Hyperparameters\n","    d_model = 16  # Small dimension for demonstration\n","    nhead = 2\n","    d_ff = d_model  # Feed-forward dimension\n","    dropout = 0.1\n","\n","    # Initialize components\n","    pos_encoder = PositionalEncoding(d_model, max_length)\n","    masked_self_attn = MaskedAttention(d_model, nhead)\n","    feed_forward = FeedForward(d_model, d_ff, dropout)\n","\n","    print(f\"Dataset configuration:\")\n","    print(f\"  Max length: {max_length}\")\n","    print(f\"  Overlap: {overlap}\")\n","    print(f\"  Total windows: {len(dataset)}\")\n","    print(f\"  Vocabulary size: {len(tokenizer)}\")\n","    print(f\"  Embedding dimension: {d_model}\")\n","    print(f\"  Number of attention heads: {nhead}\")\n","    print(f\"  Feed-forward dimension: {d_ff}\\n\")\n","\n","    # Process each window\n","    for i in range(len(dataset)):\n","        tokens, attention_mask, doc_idx = dataset[i]\n","\n","        print(f\"Window {i} (from document {doc_idx}):\")\n","        print(f\"  Original tokens: {tokens.squeeze(0).tolist()}\")\n","        print(f\"  Attention mask: {attention_mask.squeeze(0).tolist()}\")\n","        print(f\"  Decoded: '{tokenizer.decode(tokens.squeeze(0).tolist())}'\")\n","\n","        # Convert tokens to \"embeddings\" (just for demonstration)\n","        pseudo_embeddings = torch.rand(1, tokens.size(1), d_model)  # (batch_size, seq_len, d_model)\n","        print(f\"  Shape of pseudo embeddings: {pseudo_embeddings.shape}\")\n","\n","        # Apply positional encoding\n","        pos_encoded = pos_encoder(pseudo_embeddings)\n","        print(f\"  Shape after positional encoding: {pos_encoded.shape}\")\n","\n","        # Apply masked self-attention\n","        attn_output, attn_mask = masked_self_attn(pos_encoded)\n","        print(f\"  Shape after masked self-attention: {attn_output.shape}\")\n","\n","        # Apply feed-forward network\n","        ff_output = feed_forward(attn_output)\n","        print(f\"  Shape after feed-forward: {ff_output.shape}\")\n","\n","        # Display the effect of positional encoding, attention, and feed-forward for all tokens\n","        print(f\"  Transformer effect on tokens:\")\n","        for j in range(tokens.size(1)):\n","            if attention_mask[0, j] == 1:  # Only show for non-padding tokens\n","                print(f\"    Token {j}:\")\n","                print(f\"      Initial:   {pseudo_embeddings[0, j, :5].tolist()}\")\n","                print(f\"      Positional:{pos_encoded[0, j, :5].tolist()}\")\n","                print(f\"      Attention Mask: {attn_mask[j, :5].tolist()}\")  # Show first 5 values\n","                print(f\"      Attention: {attn_output[0, j, :5].tolist()}\")\n","                print(f\"      Feed-Forward: {ff_output[0, j, :5].tolist()}\")\n","        print()\n","\n","    print(\"--- End of Expanded Test ---\")\n","\n","# Run the expanded test\n","test_all_transformer_components()"],"metadata":{"id":"FyyrKkJZAuMA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 📝 Decoder Layer\n","\n","Implementation of a Transformer Decoder Layer with Masked Multi-Head Attention and Feed Forward Network\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"HM696sBjAtmY"}},{"cell_type":"markdown","source":["![Decoder Only Architecture](https://drive.google.com/uc?id=1ksROxQxf3b7dlBUoIQggzyLeBaPO-AQn)\n","\n","\n"],"metadata":{"id":"YPWxAfNbXhJ9"}},{"cell_type":"code","source":["class DecoderLayer(nn.Module):\n","    def __init__(self, d_model: int, nhead: int, d_ff: int, dropout: float = 0.1):\n","        super().__init__()\n","        self.norm1 = nn.LayerNorm(d_model)\n","\n","        #######################Code Here###############################\n","\n","        # Define the decoder - Layer Normalization, Attention, Feed_forward Dropouts\n","\n","        ###############################################################\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        # Masked Multi-Head Attention\n","        normed_x = self.norm1(x)\n","        attn_output, _ = self.masked_attention(normed_x) # _ because we returned also the mask in the previous demonstration\n","\n","        #######################Code Here###############################.\n","\n","        # Create the remaining connection and residual connections - What is a residual connection? (https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55)\n","\n","        ###############################################################\n","\n","        return x"],"metadata":{"id":"P02MAkENBZrS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 📚 Decoder-only Transformer\n","\n","The components of a Decoder-Only Transformer include an embedding layer for token representation, positional encoding for sequential information, stacked decoder layers for hierarchical processing, layer normalization for stability, and an output projection layer for generating tokens."],"metadata":{"id":"VJULRnp4BZJS"}},{"cell_type":"code","source":["class DecoderOnlyTransformer(nn.Module):\n","    def __init__(self, vocab_size: int, d_model: int, nhead: int, num_layers: int,\n","                 d_ff: int, max_seq_length: int, dropout: float = 0.1):\n","        super().__init__()\n","        self.d_model = d_model\n","        self.vocab_size = vocab_size\n","        self.max_seq_length = max_seq_length\n","\n","        # Embedding layer\n","        self.embedding = nn.Embedding(vocab_size, d_model)\n","\n","        # Positional encoding\n","        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n","\n","        # Decoder layers\n","        self.layers = nn.ModuleList([\n","            DecoderLayer(d_model, nhead, d_ff, dropout)\n","            for _ in range(num_layers)\n","        ])\n","\n","        # Final layer norm\n","        self.final_norm = nn.LayerNorm(d_model)\n","\n","        # Output projection\n","        self.output_projection = nn.Linear(d_model, vocab_size)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        # x shape: (batch_size, seq_len)\n","\n","        # Embed the input\n","        x = self.embedding(x) * math.sqrt(self.d_model)\n","\n","        # Add positional encoding\n","        x = self.positional_encoding(x)\n","\n","        # Apply decoder layers\n","        for layer in self.layers:\n","            x = layer(x)\n","\n","        # Apply final layer norm\n","        x = self.final_norm(x)\n","\n","        # Project to vocabulary size\n","        output = self.output_projection(x)\n","\n","        return output\n","\n","    def generate(self, start_tokens: torch.Tensor, max_length: int,\n","                 temperature: float = 1.0) -> torch.Tensor:\n","        self.eval()\n","        current_seq = start_tokens\n","\n","        with torch.no_grad():\n","            for _ in range(max_length - start_tokens.size(1)):\n","                # Ensure we're not exceeding the maximum sequence length\n","                if current_seq.size(1) > self.max_seq_length:\n","                    current_seq = current_seq[:, -self.max_seq_length:]\n","\n","                # Get model predictions\n","                logits = self(current_seq)\n","                next_token_logits = logits[:, -1, :] / temperature\n","\n","                # Sample next token\n","                probs = F.softmax(next_token_logits, dim=-1)\n","                next_token = torch.multinomial(probs, num_samples=1)\n","\n","                # Append next token to sequence\n","                current_seq = torch.cat([current_seq, next_token], dim=1)\n","\n","                # Check if we've generated an EOS token\n","                if next_token.item() == self.vocab_size - 1:  # Assuming EOS is the last token\n","                    break\n","\n","        return current_seq"],"metadata":{"id":"hs7mWseS3hWV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ✍ Displaying the Decoder-only Transformer Architecture"],"metadata":{"id":"YyhgXe9X7ytF"}},{"cell_type":"code","source":["!pip install torchinfo\n","from torchinfo import summary\n","\n","# Initialize the model with some example parameters\n","vocab_size = 10000\n","d_model = 512\n","nhead = 2\n","num_layers = 1\n","d_ff = 2048\n","max_seq_length = 1024\n","dropout = 0.1\n","\n","# Define your model\n","model = DecoderOnlyTransformer(\n","    vocab_size=vocab_size,\n","    d_model=d_model,\n","    nhead=nhead,\n","    num_layers=num_layers,\n","    d_ff=d_ff,\n","    max_seq_length=max_seq_length,\n","    dropout=dropout\n",")\n","\n","# Print the model summary\n","summary(model, input_size=(1, max_seq_length), dtypes=[torch.int64])"],"metadata":{"id":"49nYvRll78Ug"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 📚 Training the Decoder-only Transformer"],"metadata":{"id":"qJMRxOoSB0Go"}},{"cell_type":"code","source":["# Load the tiny_shakespeare dataset\n","dataset = load_dataset(\"tiny_shakespeare\", split=\"train\")\n","# Load the tiny_shakespeare dataset\n","# dataset = load_dataset(\"lyimo/shakespear\", split=\"train\")\n","\n","# Extract the text from the dataset\n","texts = dataset[\"text\"]\n","\n","# Hyperparameters\n","d_model = 256\n","nhead = 2\n","num_layers = 2\n","d_ff = 256\n","max_seq_length = 128\n","batch_size = 64\n","num_epochs = 10\n","learning_rate = 0.0001\n","dropout = 0.2\n","\n","# Tokenize and prepare data\n","tokenizer = SimpleTokenizer()\n","tokenizer.fit(texts)\n","vocab_size = len(tokenizer.word_to_idx)\n","\n","dataset = TextDataset(texts, tokenizer, max_seq_length)\n","train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","print(f\"Vocabulary size: {vocab_size}\")\n","\n","# Device configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Create model and move to device\n","model = DecoderOnlyTransformer(vocab_size, d_model, nhead, num_layers, d_ff, max_seq_length, dropout).to(device)\n","\n","# Create optimizer and loss function\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.word_to_idx[\"<PAD>\"])\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch_idx, batch in enumerate(train_loader):\n","        optimizer.zero_grad()\n","\n","        input_seq, _, _ = batch  # Unpack batch\n","        input_seq = input_seq.squeeze(1).to(device)  # Move input to device and remove extra dimension\n","\n","        # Forward pass\n","        output = model(input_seq)\n","\n","        # Reshape output tensor\n","        output = output[:, :-1, :].contiguous().view(-1, output.size(-1))  # Shift predictions to the left\n","\n","        # Shift targets to the right (original targets)\n","        target_seq = input_seq[:, 1:].contiguous().view(-1)\n","\n","        # Compute loss\n","        loss = criterion(output, target_seq)\n","\n","        # Debugging prints\n","        print(f\"Loss: {loss.item()}\")\n","\n","        # Backward pass and optimize\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","        if batch_idx == 0:\n","          # Debugging prints\n","          print(f\"Epoch: {epoch+1}, Batch: {batch_idx+1}\")\n","          print(f\"Input sequence shape: {input_seq.shape}\")\n","          print(f\"Input sequence: {input_seq.unsqueeze(1)}\")\n","          print(f\"Output shape before reshape: {output.shape}\")\n","          print(f\"Output shape after reshape: {output.shape}\")\n","          print(f\"Target sequence shape: {target_seq.shape}\")\n","\n","    # Print epoch loss\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader):.4f}\")"],"metadata":{"id":"gSUt8j7yBrcP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ✍ Testing the Decoder-only Transformer"],"metadata":{"id":"XEkV3lWgCsHH"}},{"cell_type":"code","source":["texts = [\"Better three hours too soon than\", \" I believe I can \", \"My words fly up, my\", \"Brevity is \", \"Love looks not with the eyes, but\", \"To be or \"]\n","\n","for quote in texts:\n","  start_tokens = torch.tensor(tokenizer.encode(quote)).unsqueeze(0).to(device)  # Add batch dimension and move to device\n","\n","  generated_tokens = model.generate(start_tokens, max_length=20, temperature=.9)\n","  generated_text = tokenizer.decode(generated_tokens.squeeze().tolist())\n","\n","  print(generated_text)"],"metadata":{"id":"ioC249m3-X2H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["_____________________________________________"],"metadata":{"id":"6_9fxH9cKaGq"}},{"cell_type":"markdown","source":["# Decoder-only with MoE instead of FFN\n","\n","In the Sparse Mixture of Experts (MoE) architecture, the self-attention mechanism within each transformer block stays the same.\n","\n","However, a key modification is made to the structure **of each block**: the standard **feed-forward neural network** is replaced with **multiple sparsely activated feed-forward networks, known as experts.**\n","\n","\"Sparse activation\" means that each token in the sequence is routed to only a small number of these experts—usually one or two—out of the entire pool.\n","\n","\n","\n","**✨ Additional Resources:**\n","\n","*   makeMoE: Implement a Sparse Mixture of Experts Language Model from Scratch [Link-huggingface](https://huggingface.co/blog/AviSoori1x/makemoe-from-scratch)\n","\n","\n"],"metadata":{"id":"LYtAnvTb83kj"}},{"cell_type":"markdown","source":["_____________________________________________"],"metadata":{"id":"Um0oVSr_KdMV"}},{"cell_type":"markdown","source":["## 📝 Expert Layer"],"metadata":{"id":"uDOiZz6LKT69"}},{"cell_type":"code","source":["class Expert(nn.Module):\n","    \"\"\" An MLP with a single hidden layer and ReLU activation, serving as an expert in a Mixture of Experts. \"\"\"\n","    def __init__(self, n_embd: int, dropout: float = 0.1):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","\n","        #######################Code Here###############################\n","\n","        # Define a Linear, ReLU, Linaer and Dropout, Linear should be (n_embd, 4 * n_embd)\n","\n","        ###############################################################\n","\n","      )\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        return self.net(x)"],"metadata":{"id":"S9PuG3xQKZcp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 📚 Gating in MoE Architectures\n","\n","Types of gating in Mixture of Experts (MoE) systems include Top-k gating, Noisy Top-k gating (as implemented here), and other variants like Hierarchical gating or Soft gating.\n","\n","Gating is essential in MoE systems because it determines which experts to use for each input, allowing the model to specialize different experts for different types of inputs or tasks.\n","\n","Specifically, Noisy Top-k gating adds controlled randomness to the expert selection process, which can help balance expert utilization and potentially improve model performance by introducing exploration in the routing mechanism."],"metadata":{"id":"KhIocpeCKevX"}},{"cell_type":"code","source":["class NoisyTopkRouter(nn.Module):\n","    def __init__(self, n_embed, num_experts, top_k_moe):\n","        super(NoisyTopkRouter, self).__init__()\n","        # Store the top_k_moe parameter which specifies the number of top experts to select\n","        self.top_k_moe = top_k_moe\n","        # Linear layer to compute logits for routing\n","        self.topkroute_linear = nn.Linear(n_embed, num_experts)\n","        # Linear layer to compute noise logits for added noise\n","        self.noise_linear = nn.Linear(n_embed, num_experts)\n","\n","    def forward(self, mh_output):\n","        # Compute the logits for routing to experts\n","        logits = self.topkroute_linear(mh_output)\n","        # Compute the noise logits\n","        noise_logits = self.noise_linear(mh_output)\n","        # Generate noise with standard deviation determined by softplus of noise logits\n","        noise = torch.randn_like(logits) * F.softplus(noise_logits)\n","        # Add noise to the original logits to get noisy logits\n","        noisy_logits = logits + noise\n","        # Select the top k logits and their indices from the noisy logits\n","        top_k_moe_logits, indices = noisy_logits.topk(self.top_k_moe, dim=-1)\n","        # Create a tensor full of -inf values\n","        zeros = torch.full_like(noisy_logits, float('-inf'))\n","        # Scatter the top k logits into the zeros tensor to create a sparse logits tensor\n","        sparse_logits = zeros.scatter(-1, indices, top_k_moe_logits)\n","        # Apply softmax to the sparse logits to get the final router output\n","        router_output = F.softmax(sparse_logits, dim=-1)\n","        # Return the router output and the indices of the selected experts\n","        return router_output, indices"],"metadata":{"id":"N1z8IuwGKjV3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 📚 Sparse MoE Layer"],"metadata":{"id":"OXoO2q9IKlmc"}},{"cell_type":"code","source":["class SparseMoE(nn.Module):\n","    def __init__(self, n_embed, num_experts, top_k_moe):\n","        super(SparseMoE, self).__init__()\n","        # Initialize the NoisyTopkRouter to determine which experts to activate\n","        self.router = NoisyTopkRouter(n_embed, num_experts, top_k_moe)\n","        # Create a list of expert networks, each being a feed-forward network\n","        self.experts = nn.ModuleList([Expert(n_embed) for _ in range(num_experts)])\n","        # Store the number of top experts to activate\n","        self.top_k_moe = top_k_moe\n","\n","    def forward(self, x):\n","        # Get the gating output and indices from the router\n","        gating_output, indices = self.router(x)\n","        # Initialize the final output tensor with zeros, having the same shape as x\n","        final_output = torch.zeros_like(x)\n","        # Flatten the input tensor to simplify processing\n","        flat_x = x.view(-1, x.size(-1))\n","        # Flatten the gating output tensor to align with the flattened input\n","        flat_gating_output = gating_output.view(-1, gating_output.size(-1))\n","\n","        # Iterate over each expert\n","        for i, expert in enumerate(self.experts):\n","            # Create a mask to identify where the current expert is used\n","            expert_mask = (indices == i).any(dim=-1)\n","            # Flatten the expert mask to match the flattened input\n","            flat_mask = expert_mask.view(-1)\n","\n","            if flat_mask.any():  # Check if there are any positions using the current expert\n","                # Extract the inputs for the current expert based on the mask\n","                expert_input = flat_x[flat_mask]\n","                # Get the output from the current expert\n","                expert_output = expert(expert_input)\n","                # Get the gating scores for the current expert\n","                gating_scores = flat_gating_output[flat_mask, i].unsqueeze(1)\n","                # Compute the weighted output based on the gating scores\n","                weighted_output = expert_output * gating_scores\n","                # Add the weighted output to the final output tensor\n","                final_output[expert_mask] += weighted_output.view_as(final_output[expert_mask])\n","\n","        # Return the final output tensor which combines the results from all activated experts\n","        return final_output\n"],"metadata":{"id":"N6kQTACyKvhF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 📝 Decoder with Sparse MoE\n","\n","![Decoder Only Architecture](https://drive.google.com/uc?id=1ksROxQxf3b7dlBUoIQggzyLeBaPO-AQn)\n","\n"],"metadata":{"id":"pru3rAF3Kwg7"}},{"cell_type":"code","source":["class DecoderLayerMoE(nn.Module):\n","    def __init__(self, d_model, nhead, d_ff, num_experts, top_k_moe, dropout=0.1):\n","        super().__init__()\n","        #######################Code Here###############################\n","\n","        # Create the Decoder architecture as before, but now add the MoE block instead of the FFN\n","\n","\n","    def forward(self, x):\n","\n","        return x\n","\n","        ###############################################################\n","\n","    def generate_square_subsequent_mask(self, sz):\n","        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n","        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","        return mask"],"metadata":{"id":"t38S7EsDK3_r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ✍ Showcase how Sparse MoE handles its inputs\n","\n","-  Experiment: Change the number of experts in the SparseMoE_example model.\n","  -  Observation: Observe how increasing or decreasing the number of experts affects the routing, gating outputs, and final output.\n","\n","- Experiment: Adjust the top_k_moe parameter to select more or fewer top experts.\n","  - Observation: See how the number of experts activated for each token changes and how it impacts the final output.\n","\n"],"metadata":{"id":"ZGu0o0A00f3t"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class SparseMoE_example(nn.Module):\n","    def __init__(self, n_embed, num_experts, top_k_moe):\n","        super(SparseMoE_example, self).__init__()\n","        self.router = NoisyTopkRouter(n_embed, num_experts, top_k_moe)\n","        self.experts = nn.ModuleList([Expert(n_embed) for _ in range(num_experts)])\n","        self.top_k_moe = top_k_moe\n","\n","    def forward(self, x):\n","        gating_output, indices = self.router(x)\n","        final_output = torch.zeros_like(x)\n","        flat_x = x.view(-1, x.size(-1))\n","        flat_gating_output = gating_output.view(-1, gating_output.size(-1))\n","\n","        for i, expert in enumerate(self.experts):\n","            expert_mask = (indices == i).any(dim=-1)\n","            flat_mask = expert_mask.view(-1)\n","\n","            if flat_mask.any():\n","                expert_input = flat_x[flat_mask]\n","                expert_output = expert(expert_input)\n","                gating_scores = flat_gating_output[flat_mask, i].unsqueeze(1)\n","                weighted_output = expert_output * gating_scores\n","                final_output[expert_mask] += weighted_output.view_as(final_output[expert_mask])\n","\n","        return final_output\n","\n","    def forward_debug_example(self, x):\n","        # Forward pass with debug prints\n","        gating_output, indices = self.router(x)\n","        print(\"Gating Output Shape:\", gating_output.shape)\n","        print(\"Gating Output:\", gating_output)\n","        print(\"Expert Indices Shape:\", indices.shape)\n","        print(\"Expert Indices:\", indices)\n","\n","        print(\"Input Shape:\", x.shape)\n","        print(\"Input:\", x)\n","\n","        final_output = torch.zeros_like(x)\n","        flat_x = x.view(-1, x.size(-1))\n","        flat_gating_output = gating_output.view(-1, gating_output.size(-1))\n","\n","        for i, expert in enumerate(self.experts):\n","            print(\"\\n\" + \"-\"*50)\n","            expert_mask = (indices == i).any(dim=-1)\n","            flat_mask = expert_mask.view(-1)\n","\n","            if flat_mask.any():\n","                expert_input = flat_x[flat_mask]\n","                print(f\"Expert {i} Input Shape:\", expert_input.shape)\n","                print(f\"Expert {i} Input:\", expert_input)\n","\n","                expert_output = expert(expert_input)\n","                print(f\"Expert {i} Output Shape:\", expert_output.shape)\n","                print(f\"Expert {i} Output:\", expert_output)\n","\n","                gating_scores = flat_gating_output[flat_mask, i].unsqueeze(1)\n","                print(f\"Gating Scores for Expert {i}:\")\n","                print(gating_scores.squeeze())\n","\n","                print(f\"Weighted Output for Expert {i}:\")\n","                weighted_output = expert_output * gating_scores\n","                print(weighted_output)\n","\n","                final_output[expert_mask] += weighted_output.view_as(final_output[expert_mask])\n","                print(f\"Expert {i} final_output Shape:\", final_output[expert_mask].shape)\n","                print(f\"Expert {i} final_output:\", final_output[expert_mask])\n","\n","        print(\"Final MoE Output Shape:\", final_output.shape)\n","        print(\"Final MoE Output:\", final_output)\n","        print(\"-\"*50)\n","        return final_output\n","\n","\n","# Example usage and debugging prints\n","def test_sparse_moe():\n","    # Parameters\n","    batch_size = 2   #\n","    seq_length = 1   # number of tokens, if 1 then it is easier to see which experts are activated and how each embedding is calculated\n","    n_embed = 5\n","    num_experts = 6  # Increased number of experts\n","    top_k_moe = 2   # If you modify, more or less experts will be activated for each input token\n","\n","    # Random input tensor (simulating token embeddings)\n","    random_input = torch.randn(batch_size, seq_length, n_embed)\n","\n","    # Initialize SparseMoE\n","    sparse_moe = SparseMoE_example(n_embed, num_experts, top_k_moe)\n","\n","    # Forward pass with debugging example\n","    final_output = sparse_moe.forward_debug_example(random_input)\n","\n","    print(\"\\nRandom Input Tensor:\")\n","    print(random_input)\n","    print(\"\\nFinal Output Tensor (after MoE processing):\")\n","    print(final_output)\n","\n","# Run the test function\n","test_sparse_moe()"],"metadata":{"id":"9DBAPMat0oJh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 📚 Decoder-only Transformer with MoE"],"metadata":{"id":"5BfeO9sAK5YD"}},{"cell_type":"code","source":["class DecoderOnlyTransformerMoE(nn.Module):\n","    def __init__(self, vocab_size, d_model, nhead, num_layers, d_ff, max_seq_length, dropout, num_experts, top_k_moe):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, d_model)\n","        self.pos_encoder = PositionalEncoding(d_model, max_seq_length)\n","        self.layers = nn.ModuleList([DecoderLayerMoE(d_model, nhead, d_ff, num_experts, top_k_moe, dropout) for _ in range(num_layers)])\n","        self.norm = nn.LayerNorm(d_model)\n","        self.output = nn.Linear(d_model, vocab_size)\n","        self.max_seq_length = max_seq_length\n","        self.num_experts = num_experts\n","        self.top_k_moe = top_k_moe\n","        self.vocab_size = vocab_size\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        x = self.pos_encoder(x)\n","\n","        for layer in self.layers:\n","            x = layer(x)\n","\n","        x = self.norm(x)\n","        return self.output(x)\n","\n","    def generate(self, start_tokens: torch.Tensor, max_length: int, temperature: float = 1.0) -> torch.Tensor:\n","        self.eval()\n","        current_seq = start_tokens\n","\n","         with torch.no_grad():  # Disable gradient computation\n","            # Generate tokens until max_length is reached or end token is generated\n","            for _ in range(max_length - start_tokens.size(1)):\n","                # Ensure the sequence length does not exceed max_seq_length\n","                if current_seq.size(1) > self.max_seq_length:\n","                    current_seq = current_seq[:, -self.max_seq_length:]\n","\n","                # Get logits from the model\n","                logits = self(current_seq)\n","\n","                # Extract logits for the next token and scale by temperature\n","                next_token_logits = logits[:, -1, :] / temperature\n","\n","                # Compute probabilities using softmax\n","                probs = F.softmax(next_token_logits, dim=-1)\n","\n","                # Sample the next token from the probability distribution\n","                next_token = torch.multinomial(probs, num_samples=1)\n","\n","                # Append the next token to the current sequence\n","                current_seq = torch.cat([current_seq, next_token], dim=1)\n","\n","                # Stop if the end token is generated (vocab_size - 1 assumed to be the end token)\n","                if next_token.item() == self.vocab_size - 1:\n","                    break\n","\n","        # Return the generated sequence\n","        return current_seq"],"metadata":{"id":"KvbjHbTCLEuO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ✍ Displaying the Decoder-only Transformer with MoE Architecture"],"metadata":{"id":"gXwM_wnqLL4d"}},{"cell_type":"code","source":["# Initialize the model with some example parameters\n","vocab_size = 10000\n","d_model = 512\n","nhead = 2\n","num_layers = 1\n","d_ff = 2048\n","max_seq_length = 1024\n","dropout = 0.1\n","num_experts = 4\n","top_k_moe = 2\n","\n","# Define your model\n","model = DecoderOnlyTransformer(\n","    vocab_size=vocab_size,\n","    d_model=d_model,\n","    nhead=nhead,\n","    num_layers=num_layers,\n","    d_ff=d_ff,\n","    max_seq_length=max_seq_length,\n","    dropout=dropout,\n",")\n","\n","# Define your model\n","model_moe = DecoderOnlyTransformerMoE(\n","    vocab_size=vocab_size,\n","    d_model=d_model,\n","    nhead=nhead,\n","    num_layers=num_layers,\n","    d_ff=d_ff,\n","    max_seq_length=max_seq_length,\n","    dropout=dropout,\n","    num_experts=num_experts,\n","    top_k_moe=top_k_moe\n",")\n","\n","# Print the model summary\n","print(50*\"-\")\n","print(summary(model, input_size=(1, max_seq_length), dtypes=[torch.int64]))\n","print(50*\"-\")\n","print(summary(model_moe, input_size=(1, max_seq_length), dtypes=[torch.int64]))"],"metadata":{"id":"HPDbqV4ULPmA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def print_model_summary(model, input_size):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","\n","    dummy_input = torch.zeros(input_size, dtype=torch.int64).to(device)\n","\n","    def register_hook(module):\n","        def hook(module, input, output):\n","            class_name = module.__class__.__name__\n","            module_idx = len(summary)\n","            m_key = f\"{module_idx:03d} {class_name}\"\n","            summary[m_key] = {}\n","            summary[m_key][\"input_shape\"] = list(input[0].size())\n","            if isinstance(output, torch.Tensor):\n","                summary[m_key][\"output_shape\"] = list(output.size())\n","            elif isinstance(output, (tuple, list)) and len(output) > 0 and isinstance(output[0], torch.Tensor):\n","                summary[m_key][\"output_shape\"] = [list(out.size()) for out in output]\n","            else:\n","                summary[m_key][\"output_shape\"] = \"multiple outputs\"\n","            params = sum(p.numel() for p in module.parameters())\n","            summary[m_key][\"num_params\"] = params\n","\n","        if not isinstance(module, nn.Sequential) and not isinstance(module, nn.ModuleList):\n","            hooks.append(module.register_forward_hook(hook))\n","\n","    summary = {}\n","    hooks = []\n","    model.apply(register_hook)\n","    model(dummy_input)\n","    for h in hooks:\n","        h.remove()\n","\n","    print(\"----------------------------------------------------------------\")\n","    print(\"{:>20}  {:>25} {:>15}\".format(\"Layer (type)\", \"Input Shape\", \"Param #\"))\n","    print(\"================================================================\")\n","    total_params = 0\n","    total_output = 0\n","    for layer in summary:\n","        line_new = \"{:>20}  {:>25} {:>15}\".format(\n","            layer,\n","            str(summary[layer][\"input_shape\"]),\n","            \"{0:,}\".format(summary[layer][\"num_params\"]),\n","        )\n","        total_params += summary[layer][\"num_params\"]\n","        if isinstance(summary[layer][\"output_shape\"], list) and all(isinstance(i, int) for i in summary[layer][\"output_shape\"]):\n","            total_output += np.prod(summary[layer][\"output_shape\"])\n","        print(line_new)\n","    print(\"================================================================\")\n","    print(f\"Total params: {total_params:,}\")\n","    print(\"----------------------------------------------------------------\")\n","\n","vocab_size = 10000\n","d_model = 512\n","nhead = 8\n","num_layers = 1\n","d_ff = 2048\n","max_seq_length = 1024\n","dropout = 0.1\n","num_experts = 2\n","top_k_moe = 1\n","\n","# Ensure you have the correct definitions for these classes\n","# from your_model_definitions import DecoderOnlyTransformer, DecoderOnlyTransformerMoE\n","\n","model = DecoderOnlyTransformer(\n","    vocab_size=vocab_size,\n","    d_model=d_model,\n","    nhead=nhead,\n","    num_layers=num_layers,\n","    d_ff=d_ff,\n","    max_seq_length=max_seq_length,\n","    dropout=dropout,\n",")\n","\n","model_moe = DecoderOnlyTransformerMoE(\n","    vocab_size=vocab_size,\n","    d_model=d_model,\n","    nhead=nhead,\n","    num_layers=num_layers,\n","    d_ff=d_ff,\n","    max_seq_length=max_seq_length,\n","    dropout=dropout,\n","    num_experts=num_experts,\n","    top_k_moe=top_k_moe\n",")\n","\n","print(\"Summary for DecoderOnlyTransformer\")\n","print_model_summary(model, (1, max_seq_length))\n","\n","print(\"\\nSummary for DecoderOnlyTransformerMoE\")\n","print_model_summary(model_moe, (1, max_seq_length))\n"],"metadata":{"id":"MGHFvdZwpdqT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 📚 Training the Decoder-only Transformer with MoE"],"metadata":{"id":"lV4CGY2ZLo1r"}},{"cell_type":"code","source":["# Load the tiny_shakespeare dataset\n","dataset = load_dataset(\"tiny_shakespeare\", split=\"train\")\n","\n","# Extract the text from the dataset\n","texts = dataset[\"text\"]\n","\n","# Hyperparameters\n","d_model = 128\n","nhead = 2\n","num_layers = 2\n","d_ff = 256\n","max_seq_length = 64\n","batch_size = 32\n","num_epochs = 1\n","learning_rate = 0.0001\n","dropout = 0.2\n","num_experts=4\n","top_k_moe=2\n","\n","# Tokenize and prepare data\n","tokenizer = SimpleTokenizer()\n","tokenizer.fit(texts)\n","vocab_size = len(tokenizer.word_to_idx)\n","\n","dataset = TextDataset(texts, tokenizer, max_seq_length)\n","train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","print(f\"Vocabulary size: {vocab_size}\")\n","\n","# Device configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Create model and move to device\n","model_moe = DecoderOnlyTransformerMoE(vocab_size, d_model, nhead, num_layers, d_ff, max_seq_length, dropout, num_experts, top_k_moe).to(device)\n","\n","# Create optimizer and loss function\n","optimizer = torch.optim.AdamW(model_moe.parameters(), lr=learning_rate)\n","criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.word_to_idx[\"<PAD>\"])\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model_moe.train()\n","    total_loss = 0\n","    for batch_idx, batch in enumerate(train_loader):\n","        optimizer.zero_grad()\n","\n","        input_seq, _, _ = batch  # Unpack batch\n","        input_seq = input_seq.squeeze(1).to(device)  # Move input to device and remove extra dimension\n","\n","        # Forward pass\n","        output = model_moe(input_seq)\n","\n","\n","        # Reshape output tensor\n","        output = output[:, :-1, :].contiguous().view(-1, output.size(-1))  # Shift predictions to the left\n","\n","        # Shift targets to the right (original targets)\n","        target_seq = input_seq[:, 1:].contiguous().view(-1)\n","\n","\n","        # Compute loss\n","        loss = criterion(output, target_seq)\n","\n","        # Debugging prints\n","        print(f\"Loss: {loss.item()}\")\n","\n","        # Backward pass and optimize\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","        if batch_idx == 0:\n","          # Debugging prints\n","          print(f\"Epoch: {epoch+1}, Batch: {batch_idx+1}\")\n","          print(f\"Input sequence shape: {input_seq.shape}\")\n","          print(f\"Input sequence: {input_seq.unsqueeze(1)}\")\n","          print(f\"Output shape before reshape: {output.shape}\")\n","          print(f\"Output shape after reshape: {output.shape}\")\n","          print(f\"Target sequence shape: {target_seq.shape}\")\n","\n","    # Print epoch loss\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n"],"metadata":{"id":"-HgiNedeLo1s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ✍ Testing the Decoder-only Transformer with MoE"],"metadata":{"id":"Ll7zTpyWLo1s"}},{"cell_type":"code","source":["texts = [\"Better three hours too soon than\", \" I believe I can \", \"My words fly up, my\", \"Brevity is \", \"Love looks not with the eyes, but\", \"To be or \"]\n","\n","for quote in texts:\n","  start_tokens = torch.tensor(tokenizer.encode(quote)).unsqueeze(0).to(device)  # Add batch dimension and move to device\n","\n","  generated_tokens = model_moe.generate(start_tokens, max_length=20, temperature=.9)\n","  generated_text = tokenizer.decode(generated_tokens.squeeze().tolist())\n","\n","  print(generated_text)"],"metadata":{"id":"yDzYIElYLo1s"},"execution_count":null,"outputs":[]}]}