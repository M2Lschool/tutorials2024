{"cells":[{"cell_type":"markdown","source":["Natural Language Processing Tutorial\n","======\n","\n","This is the tutorial of the 2024 [Mediterranean Machine Learning Summer School](https://www.m2lschool.org/) on Natural Language Processing!\n","\n","This tutorial will explore the fundamental aspects of Natural Language Processing (NLP). Basic Python programming skills are expected.\n","Prior knowledge of standard NLP techniques (e.g. text tokenization and classification with ML) is beneficial but optional when working through the notebooks as they assume minimal prior knowledge.\n","\n","This tutorial combines detailed analysis and development of essential NLP concepts via custom (i.e. from scratch) implementations. Other necessary NLP components will be developed using PyTorch's NLP library implementations. As a result, the tutorial offers deep understanding and facilitates easy usage in future applications.\n","\n","## Outline\n","\n","* Part I: Introduction to Text Tokenization and Classification\n","  *  Text Classification: Simple Classifier\n","  *  Text Classification: Encoder-only Transformer\n","\n","* Part II: Introduction to Decoder-only Transformer and Sparse Mixture of Experts Architecture\n","  *  Text Generation: Decoder-only Transformer\n","  *  Text Generation: Decoder-only Transformer + MoE\n","\n","* Part III: Introduction to Parameter Efficient Fine-tuning\n","  *  Fine-tuning the full Pre-trained Models\n","  *  Fine-tuning using Low-Rank Adaptation of Large Language Models (LoRA)\n","\n","## Notation\n","\n","* Sections marked with [ðŸ“š] contain cells that you should read, modify and complete to understand how your changes alter the obtained results.\n","* External resources are mentioned with [âœ¨]. These provide valuable supplementary information for this tutorial and offer opportunities for further in-depth exploration of the topics covered.\n","\n","\n","## Libraries\n","\n","This tutorial leverages [PyTorch](https://pytorch.org/) for neural network implementation and training, complemented by standard Python libraries for data processing and the [Hugging Face](https://huggingface.co/) datasets library for accessing NLP resources.\n","\n","GPU access is recommended for optimal performance, particularly for model training and text generation. While all code can run on CPU, a CUDA-enabled environment will significantly speed up these processes.\n","\n","## Credits\n","\n","The tutorial is created by:\n","\n","* [Luca Herranz-Celotti](http://LuCeHe.github.io)\n","* [Georgios Peikos](https://www.linkedin.com/in/peikosgeorgios/)\n","\n","It is inspired by and synthesizes various online resources, which are cited throughout for reference and further reading.\n","\n","## Note for Colab users\n","\n","To grab a GPU (if available), make sure you go to `Edit -> Notebook settings` and choose a GPU under `Hardware accelerator`\n","\n"],"metadata":{"id":"832pEvfsciyd"}},{"cell_type":"markdown","source":["In this notebook we will show how a simple sentiment classification task can be solved using first a simple neural network in PyTorch, and then using the great Transformer encoder. Let's begin."],"metadata":{"id":"-xZzJjAao8PS"}},{"cell_type":"markdown","metadata":{"id":"dxXRV09-ixEA"},"source":["# Chapter I. Simple Architecture for Language Classification"]},{"cell_type":"markdown","metadata":{"id":"G7Wv5Dm6J7xI"},"source":["##Step 1: Load Packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7zSBW7gbJJdy","collapsed":true},"outputs":[],"source":["!pip install datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5GcxgGKuH1it"},"outputs":[],"source":["from tqdm import tqdm\n","\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","from datasets import load_dataset\n","from tokenizers import Tokenizer\n","from tokenizers.models import WordPiece\n","from tokenizers.pre_tokenizers import Whitespace\n","from tokenizers.trainers import WordPieceTrainer\n","from tokenizers.processors import BertProcessing"]},{"cell_type":"markdown","metadata":{"id":"nfnZCzvnKFxJ"},"source":["##ðŸ“š Step 2: Load a Dataset\n","We'll use the âœ¨ [HuggingFace datasets](https://huggingface.co/datasets/nyu-mll/glue) library to load a dataset to play with. Let's use the GLUE MRPC dataset for sentiment analysis. âœ¨ [GLUE](https://gluebenchmark.com/), the General Language Understanding Evaluation benchmark, is a collection of resources for training, evaluating, and analyzing natural language understanding systems. The âœ¨ [MRPC](https://www.microsoft.com/en-us/download/details.aspx?id=52398), Microsoft Research Paraphrase Corpus, is a corpus of sentence pairs automatically extracted from online news sources, with human annotations indicating whether the sentences in the pair are semantically equivalent. We will load also another dataset that we will use to create our tokenizer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n9RmyOZjZHp3"},"outputs":[],"source":["# EXERCISE: Load the GLUE MRPC dataset\n","dataset = load_dataset(\n","\n","# EXERCISE: Create the tokenizer on a different dataset than the one used for\n","# training. Load the train split of the wikitext-103-raw-v1 dataset.\n","# For speed we will use only the 100K sentences.\n","num_sentences = 100_000\n","tokenizer_dataset = load_dataset("]},{"cell_type":"markdown","metadata":{"id":"kUOcwwTJK3qu"},"source":["##ðŸ“š Step 3: Tokenize the Dataset with tokenizers\n","In order to turn long texts into numbers that can be used by the mathematics of a neural network, we have to first cut long texts into small pieces, which is called tokenization, which in turn will make the step to turn those pieces into integers very simple."]},{"cell_type":"markdown","metadata":{"id":"2elGnb5gXzXX"},"source":["Four well-known types of tokenization are Character-level, Word-level, BPE and WordPiece, the last two known as two Subword tokenizers:\n","\n","1. **Character-Level Tokenization:**\n","  - **Description:** Character-level tokenization breaks down text into individual characters. Each character, including spaces and punctuation, is treated as a separate token. Used in the old days.\n","  - **Example:** For the sentence \"Hello, world!\", character-level tokenization would result in tokens: ['H', 'e', 'l', 'l', 'o', ',', ' ', 'w', 'o', 'r', 'l', 'd', '!']\n","\n","2. **Word-Level Tokenization:**\n","  - **Description:** Word-level tokenization splits text into words based on whitespace or punctuation. Each word is considered a separate token. Used in the old days.\n","  - **Example:** For the sentence \"Hello, world!\", word-level tokenization would result in tokens: ['Hello', ',', 'world', '!']\n","3. **Byte-Level Byte Pair Encoding (Byte-level BPE):**\n","  - **Description:** Byte-level BPE tokenization operates on bytes of the input text. It uses a merge operation to gradually build a vocabulary of byte pairs, making it useful for handling multilingual texts and rare characters. Used by e.g. GPT-2, RoBERTa.\n","  - **Example:** It creates tokens based on byte pairs, such as \"b\" and \"an\" merging into a single token of \"ban\".\n","4. **WordPiece Tokenization:**\n","  - **Description:** WordPiece tokenization breaks words into smaller units. It begins with a basic vocabulary of individual characters and merges the most frequent character sequences to form new tokens. Used by e.g. BERT, DistilBERT, and Electra.\n","  - **Example:** For the word \"tokenization\", WordPiece might create tokens like \"token\", \"##ization\" where \"##\" indicates continuation.\n","\n","Better tokenizers have been developed to serve different purposes in natural language processing and generation tasks, from handling character-level nuances to efficiently managing vocabulary size and handling unseen words.\n","\n","Typically you will end up using an existing tokenizer, for example the one used by GPT-2 is relatively popular, but here we show you the steps to create one from scratch using the tokenizers library by HuggingFace."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YxIccr_FNY9g"},"outputs":[],"source":["# Set the maximal number of integers fed to the Neural Network per sentence\n","max_length = 128\n","\n","# Set the number of elements the tokenizer will create as its vocabulary\n","vocab_size = 30522\n","\n","# Initialize the tokenizer with a WordPiece model, using \"[UNK]\" for unknown tokens\n","tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n","\n","# EXERCISE: Configure the tokenizer to split input text based on whitespace, as a .pre_tokenizer\n","tokenizer.pre_tokenizer =\n","\n","# Display the dataset to be used for training the tokenizer\n","print(tokenizer_dataset)\n","train_texts = tokenizer_dataset['text']\n","\n","# EXERCISE: Train the tokenizer\n","trainer = WordPieceTrainer(vocab_size=vocab_size, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n","tokenizer.train_from_iterator(\n","\n","# Set up post-processing to handle padding and truncation as BERT inputs\n","tokenizer.post_processor = BertProcessing(\n","    (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")), # Token to mark the end of a sequence\n","    (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")) # Token to mark the beginning of a sequence\n",")\n","\n","# EXERCISE: Enable truncation to ensure long sequences do not exceed max_length\n","tokenizer.enable_\n","\n","# EXERCISE: Enable padding to ensure short sequences reach the max_length, adding the\n","# \"[PAD]\" token at the end of the sentence.\n","tokenizer.enable_"]},{"cell_type":"code","source":["# Example texts\n","texts = [\n","    \"Hello, how are you?\",\n","    \"I am fine, thank you!\",\n","    \"What about you?\",\n","    \"[MASK][CLS]\"\n","]\n","\n","# Show the effect of tokenizing random sentences\n","for text in texts:\n","    print('-'*30)\n","    print(\"Text:  \", text)\n","    output = tokenizer.encode(text, 'nice')\n","    print(\"Tokens:\", output.tokens)\n","    print(\"IDs:   \", output.ids)\n","    print(\"length:\", len(output.ids))"],"metadata":{"id":"BWlFipMxK8O1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T4ng6UL7fqZI"},"source":["Let's apply our newly created tokenizer to the dataset we want to train our Neural Network to solve."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XpZib3yWSkjq"},"outputs":[],"source":["def tokenize_function(batch):\n","    # EXERCISE: Tokenize each example in the batch\n","    tokenized_batch = tokenizer.encode_batch(list(zip(batch['sentence1'],\n","\n","    # EXERCISE: Prepare tokenized outputs in the required format\n","    tokenized_dict = {\n","        'input_ids': [ for encoding in tokenized_batch],\n","        'attention_mask': [ for encoding in tokenized_batch]\n","    }\n","\n","    return tokenized_dict\n","\n","# Tokenize the dataset\n","tokenized_datasets = dataset.map(tokenize_function, batched=True)"]},{"cell_type":"markdown","metadata":{"id":"0xqBYWUrmi6D"},"source":["Now we need to turn the list of sequences into matrices, also known as mini-batches of data, of shape (batch_size, max_length), which is the standard way to feed data to Neural Networks."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jh9XOfk8H8in"},"outputs":[],"source":["batch_size = 64\n","\n","# Convert the tokenized datasets to TensorDatasets\n","def convert_to_tensors(tokenized_dataset):\n","    input_ids = torch.tensor(tokenized_dataset['input_ids'])\n","    attention_mask = torch.tensor(tokenized_dataset['attention_mask'])\n","    # EXERCISE: convert labels to tensor too\n","    labels =\n","    return TensorDataset(input_ids, attention_mask, labels)\n","\n","train_dataset = convert_to_tensors(tokenized_datasets['train'])\n","test_dataset = convert_to_tensors(tokenized_datasets['test'])\n","\n","# Create DataLoader objects\n","# EXERCISE: set the batch_size and shuffle only the train set\n","train_dataloader = DataLoader(\n","test_dataloader = DataLoader("]},{"cell_type":"markdown","metadata":{"id":"y96Y-oxDnCB1"},"source":["## ðŸ“š Step 4: Define the Neural Network\n","\n","We start with an extremely simple Neural Network. First, each integer defined through the tokenization process is assigned a random vector that is learnable, meaning the training process will change its values. That random vector is called embedding, so each word in the sentence will be represented by a learnable vector.\n","\n","Second, since each sentence has variable length, we will take the mean of the sentence over the time axis, to end up with a representation of the sentence that is of the length of the embedding vector.\n","\n","Finally we will use a linear layer to turn that mean embedding, into two possible outcomes: one will represent the network's estimate of the sentence being negative, and the other will represent its estimate of the sentence being positive."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1KF6yOcdICzN"},"outputs":[],"source":["class SimpleModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, output_dim):\n","        super(SimpleModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.fc = nn.Linear(embedding_dim, output_dim)\n","\n","    def forward(self, input_ids, attention_mask):\n","        embedded = self.embedding(input_ids)\n","\n","        # Apply attention mask\n","        masked_embedded = embedded * attention_mask.unsqueeze(-1).float()\n","\n","        # Average the embeddings across the temporal dimension\n","        # EXERCISE: average each sentence score with the sentence length\n","        summed =\n","        counts =\n","        averaged =\n","\n","        # Pass through the fully connected layer\n","        output = self.fc(averaged)\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"dmg6RyR3pPb1"},"source":["## ðŸ“š Step 5: Train and Evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"55kGha23IGn0"},"outputs":[],"source":["def train(model, num_epochs = 2, lr=1e-3, weight_decay=0.01):\n","    # Define loss function and optimizer\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n","\n","    # Initialize the model\n","    initial_embedding_weights = model.embedding.weight.data.clone()\n","\n","    # Training loop\n","    for epoch in range(num_epochs):\n","        model.train()\n","        pbar = tqdm(train_dataloader)\n","        for batch in pbar:\n","            input_ids, attention_mask, labels = batch\n","\n","            # EXERCISE: Zero the gradients\n","            optimizer.\n","\n","            # EXERCISE: Forward pass, pass the inputs and the attention_mask\n","            outputs = model(\n","\n","            # EXERCISE: Compute loss\n","            loss = criterion(\n","\n","            # EXERCISE: Backward pass and optimization step\n","            loss.\n","            optimizer.\n","            pbar.set_description(f\"Loss: {loss.item():.2f}\")\n","\n","    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n","\n","    final_embedding_weights = model.embedding.weight.data\n","\n","    # Check if the weights have changed\n","    weights_changed = not torch.equal(initial_embedding_weights, final_embedding_weights)\n","    print(\"Embedding weights changed during training:\", weights_changed)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Lxq89uNIL4y"},"outputs":[],"source":["def evaluate(model):\n","    # Evaluation loop\n","    model.eval()\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for batch in tqdm(test_dataloader):\n","            input_ids, attention_mask, labels = batch\n","\n","            # EXERCISE: Forward pass with masks\n","            outputs = model(\n","\n","            # Get predictions\n","            _, predicted = torch.max(outputs, 1)\n","\n","            # Update accuracy\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    # EXERCISE: compute the total accuracy\n","    accuracy =\n","    print(f\"\\n\\nTest Accuracy: {accuracy * 100:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dDTjRmWUSY45"},"outputs":[],"source":["# Hyperparameters\n","vocab_size = tokenizer.get_vocab_size()\n","embedding_dim = 64\n","output_dim = 2  # Binary classification\n","\n","simple_model = SimpleModel(vocab_size, embedding_dim, output_dim)\n","\n","params = sum(p.numel() for p in simple_model.parameters() if p.requires_grad)\n","print(f'The model has {params} parameters')\n","\n","# EXERCISE: play with learning rates in the set [1e-2, 1e-3, 1e-4, 1e-5]\n","# to find the best choice\n","train(simple_model, lr=1e-4, weight_decay=0.001)\n","evaluate(simple_model)"]},{"cell_type":"markdown","metadata":{"id":"IKkBrDbJpdaN"},"source":["# Chapter II: Transformer-based Architecture for Language Classification\n","\n","âœ¨ [Transformers](https://arxiv.org/pdf/1706.03762) appeared as the best option first for language translation, replacing RNNs. Now RNNs are making a come back but Transformers are still the standard, essentially thanks to having what is known as an attention mechanism everywhere in the architecture, that allows them to be able to consider all the previous time steps, while RNNs were in theory limited by being able to see only the previous time step.\n","\n","The introduction of a typical Transformer-based classifier, like âœ¨ [BERT](https://arxiv.org/pdf/1810.04805), has to be preceeded by the introduction of the MultiHearAttention as the main ingredient, and of the PositionalEncoding and FeedForward layers used as its building blocks."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0zWvzxD_I4hm"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","\n","# Constants\n","num_heads = 8    # Number of attention heads\n","num_layers = 8   # Number of Transformer layers"]},{"cell_type":"markdown","metadata":{"id":"d_90-JTIL-Qa"},"source":["## ðŸ“š Step 1: MultiHeadAttention\n","\n","The key to the MultiHeadAttention mechanism is the softmax attention.\n","The attention scores are computed using the scaled dot-product of the Query and Key vectors. The formula is:\n","\n","$$\n","Attention(Q,K,V)=softmax\\Big(\\frac{QK^T}{\\sqrt{d_k}}\\Big) V\n","$$\n","\n","where $d_k$ is the dimensionality of the $Q,K,V$ vectors. The softmax operation ensures that the scores are normalized, and the scaling factor $\\sqrt{d_k}$ helps mitigate the issue of large dot-product values. Typically $Q,K,V$ are going to be linear projections of the same tensor."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O_HM31Lfw-u2"},"outputs":[],"source":["# Multi-Head Attention\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        assert d_model % num_heads == 0\n","        self.d_k = d_model // num_heads\n","        self.num_heads = num_heads\n","\n","        # EXERCISE: create the following 4 linear layers without bias\n","        self.linear_q =\n","        self.linear_k =\n","        self.linear_v =\n","        self.linear_out =\n","\n","    def forward(self, query, mask=None):\n","        batch_size = query.size(0)\n","\n","        # Linear projections\n","        Q = self.linear_q(query)\n","        K = self.linear_k(query)\n","        V = self.linear_v(query)\n","\n","        # Split into multiple heads\n","        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)  # (batch_size, num_heads, seq_len, d_k)\n","        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)  # (batch_size, num_heads, seq_len, d_k)\n","        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)  # (batch_size, num_heads, seq_len, d_k)\n","\n","        # Attention scores\n","        scores = torch.matmul(Q, K.transpose(-2, -1))  # (batch_size, num_heads, seq_len, seq_len)\n","\n","        # EXERCISE: divide the scores by the sqrt of\n","        scores =\n","\n","        if mask is not None:\n","            mask = mask.unsqueeze(1).unsqueeze(1)  # (batch_size, 1, 1, seq_len)\n","            scores = scores.masked_fill(mask == 0, -1e9)\n","\n","        # Attention weights\n","        # EXERCISE: apply the softmax to the scores to have the attn_weights\n","        attn_weights =\n","\n","        # Weighted sum of values\n","        attn_output = torch.matmul(attn_weights, V)  # (batch_size, num_heads, seq_len, d_k)\n","\n","        # Concatenate heads and project back to d_model\n","        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)  # (batch_size, seq_len, d_model)\n","        attn_output = self.linear_out(attn_output)  # (batch_size, seq_len, d_model)\n","\n","        return attn_output"]},{"cell_type":"markdown","metadata":{"id":"m7NxFPXtOIqU"},"source":["## ðŸ“š Step 2: PositionalEncoding and FeedForward\n","\n","Next key factors to Transformer-based architectures success are the positional encoding and the interleaved feedforward network. The PositionalEncoding adds positional information to input tokens using sinusoidal functions. The FeedForward layer is a simple feedforward neural network with two linear layers and ReLU activation, that projects the MHA representation into a 4x wider representation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WGrGgebTw_mQ"},"outputs":[],"source":["import math\n","\n","# Positional Encoding\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        position = torch.arange(max_len).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n","        pe = torch.zeros(max_len, 1, d_model)\n","        pe[:, 0, 0::2] = torch.sin(position * div_term)\n","        pe[:, 0, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:x.size(0)]\n","        return x\n","\n","# Feedforward Layer\n","class FeedForward(nn.Module):\n","    def __init__(self, d_model, d_ff):\n","        super(FeedForward, self).__init__()\n","        self.linear1 = nn.Linear(d_model, d_ff)\n","        self.activation = nn.GELU()\n","        self.linear2 = nn.Linear(d_ff, d_model)\n","\n","    def forward(self, x):\n","        # EXERCISE: x = linear(activation(linear(x)))\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"hHj74Z_NPCSH"},"source":["## ðŸ“š Step 3: Transformer Encoder\n","\n","The final architecture is a sequence of Transformer blocks where a specific sequence of LayerNormalization, Dropout, skip connections and the layers introduced above are used. Finally the Transformer blocks are chained after the embedding and the positional embedding."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F_lCluW2xqs6"},"outputs":[],"source":["# Transformer Decoder Layer\n","class TransformerEncoderLayer(nn.Module):\n","    def __init__(self, d_model, num_heads, d_ff):\n","        super(TransformerEncoderLayer, self).__init__()\n","        self.self_attn = MultiHeadAttention(d_model, num_heads)\n","\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.dropout1 = nn.Dropout(p=0.1)\n","\n","        self.ff = FeedForward(d_model, d_ff)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.dropout2 = nn.Dropout(p=0.1)\n","\n","    def forward(self, tgt, tgt_mask=None):\n","        tgt2 = self.self_attn(tgt, mask=tgt_mask)\n","\n","        # EXERCISE: norm(tgt + dropout(tgt2))\n","        tgt =\n","\n","        # EXERCISE: norm(tgt + dropout(ff(tgt)))\n","        tgt =\n","\n","        return tgt\n","\n","# Transformer Encoder\n","class TransformerEncoder(nn.Module):\n","    def __init__(self, input_dim, d_model, num_heads, num_layers, d_ff, output_dim):\n","        super(TransformerEncoder, self).__init__()\n","        self.embedding = nn.Embedding(input_dim, d_model)\n","        self.pos_encoder = PositionalEncoding(d_model)\n","\n","        # EXERCISE: a list of layers has to be recorded as a ModuleList in pytorch\n","        self.layers = [TransformerEncoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)]\n","        self.dropout = nn.Dropout(p=0.1)\n","        self.fc_out = nn.Linear(d_model, output_dim)\n","\n","    def forward(self, tgt, tgt_mask=None):\n","        # EXERCISE: do the embedding and follow them with a pos_encoder\n","        tgt =\n","        tgt =\n","\n","        for layer in self.layers:\n","            tgt = layer(tgt, tgt_mask)\n","\n","        summed = tgt.sum(1)\n","        counts = tgt_mask.sum(1, keepdim=True)\n","        tgt = summed / counts\n","\n","        output = self.fc_out(self.dropout(tgt))\n","        return output"]},{"cell_type":"markdown","source":["## ðŸ“š Step 4: Train and Evaluate"],"metadata":{"id":"fBG020UX4hBi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YNcAXxMO360Z"},"outputs":[],"source":["bert_model = TransformerEncoder(\n","    input_dim=vocab_size,\n","    d_model=embedding_dim,\n","    num_heads=num_heads,\n","    num_layers=num_layers,\n","    d_ff=4*embedding_dim,\n","    output_dim=output_dim\n",")\n","\n","params = sum(p.numel() for p in bert_model.parameters() if p.requires_grad)\n","print(f'The model has {params} parameters')\n","\n","# EXERCISE: play with learning rates in the set [1e-2, 1e-3, 1e-4, 1e-5]\n","# to find the best choice\n","train(bert_model, lr=1e-2, weight_decay=0.001)\n","evaluate(bert_model)"]},{"cell_type":"code","source":[],"metadata":{"id":"Lf_VhyEOgXuN"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}